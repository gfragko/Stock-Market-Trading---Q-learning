{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Project 2: Stock Portfolio Optimization - Assignment 3**\n",
    "# Athanasakis Evangelos 2019030118 // Fragkogiannis Yiorgos 2019030039\n",
    "\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tkinter as tk #loads standard python GUI libraries\n",
    "import random\n",
    "from tkinter import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------__________________Environments___________________-------------------------------------------------------------------------\n",
    "# Generating environments\n",
    "\n",
    "\n",
    "# Create the three different environments\n",
    "# We are modeling this environment using 8 states in the format: {stock_currently_holding,state_of_stock_1,state_of_stock_2}\n",
    "\n",
    "action_keep = 0     # keep the same stock\n",
    "action_switch = 1   # switch to the other stock\n",
    "\n",
    "# This environment is used for the question 1 where we need to demonstrate that the optimal\n",
    "# policy is always to stay with the stock we already have invested\n",
    "fee = -0.9\n",
    "# r1H = 2*r2L\n",
    "# in this case r1.h=0.1 // r2.H= 0.05 // r1.L = -0.02 // r2.L = 0.01\n",
    "# we have used a large transaction fee so that the best policy will always be to keep using the same stock\n",
    "P1 = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "            (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "            (1/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "            (9/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (1/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (9/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# This environment implements the stocks environment from the midterm\n",
    "# It is used for the question 2 where we need to demonstrate that the optimal policy\n",
    "# for some of the states is to switch and in some others to stay\n",
    "fee = -0.01\n",
    "P2 = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05  + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "            (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "            (1/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "            (9/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# This environment implements the generic scenario of question 3 where for every stock\n",
    "# ri_H,ri_L are chosen uniformly in [-0.02, 0.1] and transition probabilities pi_HL, pi_LH\n",
    "# are equal to 0.1 for half the stocks and 0.5 for the other half.\n",
    "\n",
    "# Since every stock can have two price states, the number of total states in the MDP\n",
    "# we are creating will be = NumOfStoscks*2^numOfStocks\n",
    "\n",
    "\n",
    "def decimal_to_binary_array(decimal, length):\n",
    "\n",
    "    # Convert decimal to binary string (strip '0b' prefix)\n",
    "    binary_string = bin(decimal)[2:]\n",
    "\n",
    "    # Determine padding length\n",
    "    padding_length = max(0, length - len(binary_string))\n",
    "\n",
    "    # Pad binary string with leading zeros if needed\n",
    "    padded_binary_string = '0' * padding_length + binary_string\n",
    "\n",
    "    # Convert padded binary string to list of binary digits\n",
    "    binary_array = [int(bit) for bit in padded_binary_string]\n",
    "\n",
    "    return binary_array\n",
    "\n",
    "\n",
    "# Function that generates the environment of N stocks dynamically, with a transaction fee\n",
    "def generate_environment(N,fee):\n",
    "\n",
    "    states_for_each_stock = 2**N\n",
    "    total_states = N * states_for_each_stock\n",
    "    max_state_length = N\n",
    "\n",
    "    P = {}\n",
    "    pi = []\n",
    "    #Creating transition probabilities for the keep action\n",
    "    #of EACH stock\n",
    "    for i in range(0,N):\n",
    "        if(i < N/2):\n",
    "            # pi_HL = pi_LH = 0.1 | # pi_HH = pi_LL = 0.9\n",
    "            row = [0.9,0.1,0.1,0.9] #[LL,LH,HL,HH]\n",
    "        else:\n",
    "            # pi_HL = pi_LH = 0.5 | # pi_HH = pi_LL = 0.5\n",
    "            row = [0.5,0.5,0.5,0.5] #[LL,LH,HL,HH]\n",
    "        pi.append(row)\n",
    "\n",
    "    progress_bar = tqdm(range(0, total_states))\n",
    "    for i in progress_bar:\n",
    "        SubDictionary={}\n",
    "        action_Keep = []\n",
    "        action_Switch = []\n",
    "\n",
    "        # find what stock we are reffering to\n",
    "        # Stock ids start from 0\n",
    "        stock = i // states_for_each_stock\n",
    "\n",
    "        ##########################\n",
    "        # We define states of L and H with binary ids\n",
    "        # For example for 2 stocks this translation occurs:\n",
    "        # LL -> 0,0 -> 0\n",
    "        # LH -> 0,1 -> 1\n",
    "        # HL -> 1,0 -> 2\n",
    "        # HH -> 1,1 -> 3\n",
    "        # The binary ids are then translated to decimals so that\n",
    "        # we can use them in code\n",
    "        ##########################\n",
    "\n",
    "        current_state = i - stock * states_for_each_stock # find where this specific stock starts at the total_states environment\n",
    "                                                          # this is necessary to calculate the transition probabilities\n",
    "\n",
    "        # Convert decimal to binary string\n",
    "        # Convert the binary string to a list of integers (0s and 1s)\n",
    "        curr_state_array = decimal_to_binary_array(current_state, max_state_length)\n",
    "        # We can now use the array to find if each stock is in high (1s) or low (0s) state\n",
    "        # So We now know that we are at state {x,L,L,H....,H} with x the number of current stock\n",
    "\n",
    "        #__Keep Stock ________________________________________________________________________________________________________________\n",
    "        # progress_1 = tqdm(range (stock*2**N, ((stock+1)*2**N)))\n",
    "        for j in range (stock*2**N, ((stock+1)*2**N)): # for every possible transition when keeping the same stock\n",
    "            state_to_trans = j - stock * states_for_each_stock          # value (H or L) of all of the stocks at the state we will transition to, in decimal form (0,1,2,3...)\n",
    "            trans_state_array = decimal_to_binary_array(state_to_trans, max_state_length) # convert to binary and take each bit separately (0 for L and 1 for H)\n",
    "\n",
    "            transitionProb = 1\n",
    "\n",
    "            for k in range(len(trans_state_array)):\n",
    "                stock_state_trans = trans_state_array[k] # 0 or 1 // low or high\n",
    "                stock_state_current = curr_state_array[k] # 0 or 1 // low or high\n",
    "\n",
    "                if(stock_state_current == 0 and stock_state_trans == 0):       # Pi_LL\n",
    "                    transitionProb = transitionProb * pi[stock][0]\n",
    "                elif(stock_state_current == 0 and stock_state_trans == 1):     # pi_LH\n",
    "                    transitionProb = transitionProb * pi[stock][1]\n",
    "                elif(stock_state_current == 1 and stock_state_trans == 0):     # pi_HL\n",
    "                    transitionProb = transitionProb * pi[stock][2]\n",
    "                else:                                                          # pi_HH\n",
    "                    transitionProb = transitionProb * pi[stock][3]\n",
    "\n",
    "            nextState = j\n",
    "            #reward = random.uniform(-0.02, 20)\n",
    "            reward = random.uniform(-0.02, 0.1)\n",
    "            action_Keep.append((transitionProb,nextState,reward))\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        #fee = 0\n",
    "        #__Switch Stock ________________________________________________________________________________________________________________\n",
    "        # progress_bar = tqdm(range (0, total_states))\n",
    "        for j in range (0, total_states): # for every possible transition when keeping the same stock\n",
    "            trans_stock = j // states_for_each_stock\n",
    "\n",
    "            if(trans_stock == stock):     # check if the transition stock is the same as the stock we start from\n",
    "                continue                  # we have already handle this situation above so we move on\n",
    "\n",
    "\n",
    "            trans_state = j - trans_stock * states_for_each_stock\n",
    "            trans_state_array = decimal_to_binary_array(trans_state, max_state_length)\n",
    "            transitionProb = 1\n",
    "\n",
    "            for k in range(len(trans_state_array)):\n",
    "                stock_state_trans = trans_state_array[k] # 0 or 1 // low or high\n",
    "                stock_state_current = curr_state_array[k] # 0 or 1 // low or high\n",
    "\n",
    "                if(stock_state_current == 0 and stock_state_trans == 0):       # Pi_LL\n",
    "                    transitionProb = transitionProb * pi[stock][0]\n",
    "                elif(stock_state_current == 0 and stock_state_trans == 1):     # pi_LH\n",
    "                    transitionProb = transitionProb * pi[stock][1]\n",
    "                elif(stock_state_current == 1 and stock_state_trans == 0):     # pi_HL\n",
    "                    transitionProb = transitionProb * pi[stock][2]\n",
    "                else:                                                          # pi_HH\n",
    "                    transitionProb = transitionProb * pi[stock][3]\n",
    "\n",
    "            nextState = j\n",
    "            #reward = random.uniform(-0.02, 20) - fee\n",
    "            reward = random.uniform(-0.02, 0.1) - fee\n",
    "            action_Switch.append((transitionProb,nextState,reward))\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        SubDictionary[action_keep] = action_Keep\n",
    "        SubDictionary[action_switch] = action_Switch\n",
    "        P[i]=SubDictionary\n",
    "\n",
    "\n",
    "\n",
    "    return P\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1, Policy Evaluation/Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(pi, P, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
    "    #print(\"in policy EVALUATION\")\n",
    "    t = 0   #there's more elegant ways to do this\n",
    "    prev_V = np.zeros(len(P)) # use as \"cost-to-go\", i.e. for V(s')\n",
    "    while True:\n",
    "        V = np.zeros(len(P)) # current value function to be learnerd\n",
    "        for s in range(len(P)):  # do for every state\n",
    "            for prob, next_state, reward in P[s][pi(s)]:  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
    "                V[s] = np.int64(V[s] + prob * (reward + gamma * prev_V[next_state]))\n",
    "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one;     \n",
    "            break # if yes, finish loop\n",
    "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
    "        t += 1\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(V, P, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
    "    #print(\"in policy IMPROVEMENT\")\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64) #create a Q value array\n",
    "    for s in range(len(P)):        # for every state in the environment/model\n",
    "        for a in range(len(P[s])):  # and for every action in that state\n",
    "            for prob, next_state, reward in P[s][a]:  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve) \n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state])\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
    "    # lambda is a \"fancy\" way of creating a function without formally defining it (e.g. simply to return, as here...or to use internally in another function)\n",
    "    # you can implement this in a much simpler way, by using just a few more lines of code -- if this command is not clear, I suggest to try coding this yourself\n",
    "    \n",
    "    return new_pi,Q\n",
    "\n",
    "# policy iteration is simple, it will call alternatively policy evaluation then policy improvement, till the policy converges.\n",
    "\n",
    "def policy_iteration(P, gamma = 1.0, epsilon = 1e-10):\n",
    "    t = 0\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))     # start with random actions for each state  \n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
    "    #print(\"Policy in first iteration:\")\n",
    "    #print_policy(pi,len(P))\n",
    "    #print(\"\\n\")\n",
    "    while True:\n",
    "        old_pi = {s: pi(s) for s in range(len(P))}  #keep the old policy to compare with new\n",
    "        V = policy_evaluation(pi,P,gamma,epsilon)   #evaluate latest policy --> you receive its converged value function\n",
    "        pi,Q_table = policy_improvement(V,P,gamma)          #get a better policy using the value function of the previous one just calculated \n",
    "        \n",
    "        t += 1    \n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}: # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
    "            break\n",
    "    print('Converged after %d Policy Iterations' %t) #keep track of the number of (outer) iterations to converge\n",
    "    return V,pi,Q_table\n",
    "\n",
    "\n",
    "# Function to print policy\n",
    "def print_policy(policy, num_states=8):\n",
    "    for s in range(num_states):\n",
    "        print(f\"State {s}: Action {policy(s)}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Functions for Tubular Qlearning and DQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference_and_mse(q1, q2):\n",
    "    # Ensure the tables have the same dimensions\n",
    "    if len(q1) != len(q2) or any(len(row1) != len(row2) for row1, row2 in zip(q1, q2)):\n",
    "        raise ValueError(\"Both tables must have the same dimensions.\")\n",
    "    \n",
    "    result = []\n",
    "    total_squared_error = 0\n",
    "    num_elements = 0\n",
    "    \n",
    "    for row1, row2 in zip(q1, q2):\n",
    "        row_diff = []\n",
    "        for element1, element2 in zip(row1, row2):\n",
    "            diff = element1 - element2\n",
    "            row_diff.append(diff)\n",
    "            total_squared_error += diff ** 2\n",
    "            num_elements += 1\n",
    "        result.append(row_diff)\n",
    "    \n",
    "    mse = total_squared_error / num_elements\n",
    "    return result, mse\n",
    "\n",
    "\n",
    "def check_q_table_convergence(prev_Q, current_Q, epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Checks if the Q-table has converged.\n",
    "\n",
    "    Parameters:\n",
    "    - prev_Q (np.ndarray): Previous Q-table.\n",
    "    - current_Q (np.ndarray): Current Q-table.\n",
    "    - epsilon (float): Convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if Q-table has converged, False otherwise.\n",
    "    \"\"\"\n",
    "    if prev_Q is None:\n",
    "        return False  # Cannot determine convergence without a previous Q-table\n",
    "    \n",
    "    # Calculate the maximum absolute difference between corresponding Q-values\n",
    "    max_diff = np.max(np.abs(prev_Q - current_Q))\n",
    "    \n",
    "    # Check if the maximum difference is less than epsilon\n",
    "    if max_diff < epsilon:\n",
    "        return True  # Q-table has converged\n",
    "    \n",
    "    return False  # Q-table has not converged yet\n",
    "\n",
    "\n",
    "# This function is used to simulate the environments response\n",
    "# It gets as input the environment, the current state and the action that we have selected\n",
    "# and it returns the next state and the reward\n",
    "def get_response(environment, state, action):\n",
    "    P = environment\n",
    "    \n",
    "    response = P[state][action] # get next states, transition probabilities and transaction rewards\n",
    "                                # based on the current state and the action we want to make   \n",
    "\n",
    "    # we use random.choices to get a random next state based on the weighted probabilities of the next states\n",
    "    probabilities = []\n",
    "    choices = range(len(P[state][action]))\n",
    "    for i in range(len(P[state][action])): \n",
    "        probabilities.append(response[i][0])\n",
    "        \n",
    "     \n",
    "    # because depending on the action (keep or switch) the num of actions we can take is different\n",
    "    # hence, we check what the action we do is and declare the choices array accordingly\n",
    "        \n",
    "    # Make a random choice based on probabilities\n",
    "    # k=1: Specifies that we want to make a single random choice.\n",
    "    # [0] is used to extract the single element from that list\n",
    "    random_choice = random.choices(choices, weights=probabilities, k=1)[0]\n",
    "     \n",
    "    next_state = response [random_choice][1] # get next state\n",
    "    reward = response [random_choice][2]     # get reward\n",
    "     \n",
    "    return next_state,reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2\n",
    " Implementing Tubular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================================================\n",
    "#################### Q-Learning ################\n",
    "#===== Hyperparameters ===================\n",
    "# alpha -> Learning rate\n",
    "# gamma -> Discount factor\n",
    "# epsilon ->  # Exploration rate\n",
    "# epsilon_decay -> Decay rate for epsilon\n",
    "# min_epsilon -> Minimum epsilon value\n",
    "# num_episodes -> Number of episodes\n",
    "\n",
    "def implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters):\n",
    "    Q = np.zeros((len(environment),len(environment[0])))\n",
    "    epsilon = 1.0               # Exploration rate0\n",
    "    #epsilon_decay = 0.99        # Decay rate for epsilon\n",
    "    min_epsilon = 0.1           # Minimum epsilon value\n",
    "    #alpha_decay = 0.01\n",
    "    initial_alpha = alpha\n",
    "    min_alpha = 0.001\n",
    "    convergence_episode = float('inf')  # Initialize with a large number\n",
    "    conv_counter = 0\n",
    "\n",
    "    progress_bar = tqdm(range(num_of_episodes))\n",
    "    for episode in progress_bar: \n",
    "        prev_Q = np.copy(Q)\n",
    "        current_state = random.randint(0, len(environment)-1) # select a random starting state\n",
    "        \n",
    "        for _ in range(100):      # do 100 steps do get a feel for what happens in the environment\n",
    "            # decide if we are going to explore or to exploit based on the epsilon value\n",
    "            if random.uniform(0,1) < epsilon:\n",
    "                # Explore by picking a random action\n",
    "                action = random.choice([0,1])\n",
    "            else:\n",
    "                action = np.argmax(Q[current_state])\n",
    "\n",
    "            next_state,reward = get_response(environment, current_state, action)\n",
    "            \n",
    "            Q[current_state,action] = Q[current_state,action] + alpha * (\n",
    "                reward + gamma * np.max(Q[next_state]) - Q[current_state,action]\n",
    "            )\n",
    "            \n",
    "            # update the current state\n",
    "            current_state = next_state    \n",
    "        # update the hyperparameters     \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        alpha = max(min_alpha, initial_alpha * np.exp(-alpha_decay * episode))\n",
    "        \n",
    "        \n",
    "        if finding_parameters == True and check_q_table_convergence(prev_Q, Q, epsilon=0.00002):\n",
    "            conv_counter += 1\n",
    "            if conv_counter > 2:  # Adjust convergence criteria based on your problem\n",
    "                # convergence_episode = min(convergence_episode, episode)\n",
    "                convergence_episode = episode\n",
    "                # print(\"prev_Q:\", prev_Q)\n",
    "                # print(\"Q:\", Q)\n",
    "                print(\"convergence_episode = \",convergence_episode)\n",
    "                # print(np.argmax(Q,axis=1))\n",
    "                conv_counter = 0\n",
    "                break\n",
    "\n",
    "    # print(\"\\n\",Q)\n",
    "    return Q, convergence_episode\n",
    "\n",
    "\n",
    "\n",
    "# environment = P2\n",
    "# alpha = 0.5\n",
    "# gamma = 0\n",
    "# V_opt1,P_opt1,Q_opt = policy_iteration(environment,gamma)\n",
    "\n",
    "\n",
    "# Define objective function for Optuna\n",
    "# Optuna tries to minimise the output of the objective function by modifying the hyperparameters of the tubular q learning algorithm\n",
    "# The output of the function will be the mse of the policy found at convergence summed up with the number of steps it took to converge\n",
    "# Because finding a correct policy is more important then the number of steps, mse is (weighted) multiplied with 10^14 (so that it has greater impact on \n",
    "# the output of the objective function)\n",
    "def objective(trial):    \n",
    "    environment = P2  # Define your environment here\n",
    "    num_of_episodes = 10000  # Adjust as needed\n",
    "    alpha = trial.suggest_float('alpha', 0.5, 0.9, log=True)\n",
    "    gamma = 0\n",
    "    epsilon_decay = trial.suggest_float('epsilon_decay', 0.95, 0.999)\n",
    "    alpha_decay = trial.suggest_float('alpha_decay', 0.001, 0.01)\n",
    "    finding_parameters = True    \n",
    "    Q, convergence_episode = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\n",
    "    print(np.argmax(Q,axis=1))\n",
    "    \n",
    "    # Return the inverse of convergence episode (maximize speed)\n",
    "    convergence_episode = convergence_episode if convergence_episode != float('inf') else 10000\n",
    "    r, mse = calculate_difference_and_mse(Q_opt, Q)   \n",
    "    difference_count = sum(1 for x, y in zip(np.argmax(Q_opt,axis=1), np.argmax(Q,axis=1)) if x != y)\n",
    "    result = mse * 10000000000000000 * (difference_count+1) + convergence_episode/10 \n",
    "    print(\"mse: \",mse,\" result: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# def count_tables_differences(table1, table2):\n",
    "#     if len(table1) != len(table2):\n",
    "#         raise ValueError(\"Both tables must have the same length.\")\n",
    "    \n",
    "#     difference_count = sum(1 for x, y in zip(table1, table2) if x != y)\n",
    "#     return difference_count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Deep Q-Learning Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################____TASK3____########################################\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Class That Implements Our Deep Q-Network\n",
    "class stock_market_trading_DQN():    \n",
    "    # HyperParameters\n",
    "    alpha = 0.001              # Learning rate\n",
    "    gamma = 0              # Discount Factor\n",
    "    synching_period = 100    # After this many batches we synch the target nn with the policy nn\n",
    "    replay_buffer_size = 10000 # Size of replay buffer\n",
    "    min_batch_size = 64      # Size of each batch\n",
    "    #optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "\n",
    "    # Define Huber as our loss function\n",
    "    # loss_func = nn.SmoothL1Loss()\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    ACTIONS = [0,1]\n",
    "    num_actions = 2\n",
    "    \n",
    "    # Encode the input state \n",
    "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states)\n",
    "        input_tensor[state] = 1\n",
    "        return input_tensor\n",
    "            \n",
    "    # This method is responsible to train our network based on a number of 'episodes'\n",
    "    def train_DQN(self, episodes,environment,gamma,lr):\n",
    "        P = environment\n",
    "        num_of_states = len(P)\n",
    "        num_of_actions = len(P[0])\n",
    "        \n",
    "        epsilon = 1 # Exploration rate\n",
    "        self.gamma = gamma\n",
    "        self.alpha = lr\n",
    "        memory_buffer = ReplayMemory(self.replay_buffer_size)\n",
    "        #memory_buffer = [[] for _ in range(self.replay_buffer_size)] \n",
    "        \n",
    "        #memory_buffer[i % 1000] = [0,1,2,3]\n",
    "        \n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        # We create a NN with num of input nodes equal to the num of the total states \n",
    "        # The num of output layer nodes is equal to the num of the total actions\n",
    "        # The hidden layer's num of nodes is equal to the num of states -> this is adjustable\n",
    "        policy_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions)\n",
    "        target_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions)\n",
    "\n",
    "        # initialize the 2 networks to be the same \n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # print('Policy (random, before training):')\n",
    "        # self.print_dqn(policy_dqn)\n",
    "        # print('===============================================================')\n",
    "        # print('===============================================================')\n",
    "\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        # self.optimizer = torch.optim.RMSprop(policy_dqn.parameters(), lr=self.alpha, alpha=0.99, \n",
    "        #                                      eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.alpha)\n",
    "        # optimizer = SGD([parameter], lr=0.1)\n",
    "        \n",
    "        # keep track of the reward at each round \n",
    "        reward_tracking = np.zeros(episodes)\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_tracking = []\n",
    "        synch_counter = 0 # which step we are on \n",
    "        \n",
    "        progress_bar = tqdm(range(episodes))\n",
    "        for i in progress_bar:\n",
    "            current_state = random.randint(0, len(P)-1) # select a random starting state\n",
    "        \n",
    "            for _ in range(100):      # do 100 steps do get a feel for what happens in the environment\n",
    "                # decide if we are going to explore or to exploit based on the epsilon value\n",
    "                # if random.uniform(0,1) < epsilon:\n",
    "                if random.random() < epsilon:\n",
    "                    #action = np.random.binomial(1,0.5)     # Explore by picking a random action\n",
    "                    action = random.choice([0,1])\n",
    "                else:\n",
    "                     # From the output layer, choose the node output (action) with the maximum value\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(current_state, num_of_states)).argmax().item()\n",
    "                    \n",
    "                # get the response from the environment\n",
    "                next_state,reward = get_response(P, current_state, action)\n",
    "                # reward_tracking[i] = reward\n",
    "                \n",
    "                # Store the environments response into our memory        \n",
    "                # memory_buffer[step % 1000] = [current_state, action, next_state, reward]\n",
    "                memory_buffer.append((current_state, action, next_state, reward)) \n",
    "            \n",
    "                # update the next state\n",
    "                current_state = next_state    \n",
    "            \n",
    "                # Increment step counter\n",
    "                synch_counter += 1\n",
    "            \n",
    "            # Perform the optimization\n",
    "            if(len(memory_buffer) > self.min_batch_size):\n",
    "\n",
    "                #mini_batch = self.sample_mem_buffer(memory_buffer, self.min_batch_size)\n",
    "                mini_batch = memory_buffer.sample(self.min_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                #epsilon = max(epsilon * 0.99, 0.1)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                ### CHECK\n",
    "                # if (step % self.synching_period) == 0:\n",
    "                if synch_counter > self.synching_period :\n",
    "                # if (synch_counter  self.synching_period): \n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    synch_counter = 0\n",
    "\n",
    "        # return the optimal policy\n",
    "        #return policy_dqn.state_dict()\n",
    "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
    "        return policy_dqn\n",
    "                \n",
    "    def optimize(self,mini_batch, policy_dqn, target_dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = policy_dqn.fc1.in_features\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward in mini_batch:\n",
    "            # Calculate target q value \n",
    "            # We disable the gradient tracking for memory optimization\n",
    "            with torch.no_grad():\n",
    "                # Here we get the optimal output we SHOULD have gotten according to the target NN\n",
    "                target = torch.FloatTensor(\n",
    "                    # For DQNs the target NNs parameters are modified according to the equation\n",
    "                    # Q[state,action] = reward + γ *max{Q[next_state]}\n",
    "                    reward + self.gamma * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
    "                )\n",
    "                    \n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state, num_states)) \n",
    "\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # calculate the loss for all the batch  \n",
    "        loss = self.loss_func(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model by running back-propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Test function\n",
    "    def test_DQN(self, episodes,environment):\n",
    "        # Create FrozenLake instance\n",
    "        P = environment\n",
    "        num_of_states = len(P)\n",
    "        num_of_actions = len(P[0])\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions) \n",
    " \n",
    "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        # print('Policy (trained):')\n",
    "        # self.print_dqn(policy_dqn)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            current_state = random.randint(0, num_of_states-1)\n",
    "\n",
    "            for _ in range(100):\n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(current_state, num_of_states)).argmax().item()\n",
    "                # Execute action\n",
    "                current_state,reward = get_response(P, current_state, action)\n",
    "\n",
    "        \n",
    "        \n",
    "    def print_dqn(self, dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = dqn.fc1.in_features\n",
    "        Q_table = np.zeros((num_states, self.num_actions))\n",
    "\n",
    "        # Loop each state and print policy to console\n",
    "        for s in range(num_states):\n",
    "\n",
    "            q_values_element = dqn(self.state_to_dqn_input(s, num_states)).tolist()\n",
    "            Q_table[s] = q_values_element\n",
    "            \n",
    "            #  Format q values for printing\n",
    "            q_values = ''\n",
    "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
    "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
    "            q_values=q_values.rstrip()              # Remove space at the end\n",
    "            #\n",
    "\n",
    "            # Map the best action\n",
    "            best_action = dqn(self.state_to_dqn_input(s, num_states)).argmax()\n",
    "\n",
    "            # Print policy in the format of: state, action, q values\n",
    "            # The printed layout matches the FrozenLake map.\n",
    "            print(f'{s:02},{best_action},[{q_values}]', end='\\n')         \n",
    "            if (s+1)%4==0:\n",
    "                print() # Print a newline every 4 states\n",
    "            \n",
    "        #Q_table_transposed = [list(row) for row in zip(*Q_table)]\n",
    "        return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = P2\n",
    "#environment = generate_environment(3,0.001)\n",
    "gamma = 0\n",
    "#NN_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Optimal Policy (policy Iteration -> Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 Policy Iterations\n"
     ]
    }
   ],
   "source": [
    "V_opt1,P_opt1,Q_opt = policy_iteration(environment,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Tubular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 16:48:34,242] A new study created in memory with name: no-name-00ec19ad-3a91-4ac9-8f2d-68ab1d0da562\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1122.29it/s]\n",
      "[I 2024-07-18 16:48:43,159] Trial 0 finished with value: 5474045722.588112 and parameters: {'alpha': 0.6755170782687087, 'epsilon_decay': 0.9616651564373581, 'alpha_decay': 0.009343148154535276}. Best is trial 0 with value: 5474045722.588112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  5.474044722588112e-07  result:  5474045722.588112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1098.88it/s]\n",
      "[I 2024-07-18 16:48:52,263] Trial 1 finished with value: 6209106436.590572 and parameters: {'alpha': 0.8015357949880814, 'epsilon_decay': 0.9870713210486525, 'alpha_decay': 0.0018496796489528445}. Best is trial 0 with value: 5474045722.588112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  6.209105436590573e-07  result:  6209106436.590572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1112.46it/s]\n",
      "[I 2024-07-18 16:49:01,255] Trial 2 finished with value: 13682800902.582363 and parameters: {'alpha': 0.7317239780978271, 'epsilon_decay': 0.9911889429572313, 'alpha_decay': 0.007120597364075478}. Best is trial 0 with value: 5474045722.588112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.3682799902582363e-06  result:  13682800902.582363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1105.78it/s]\n",
      "[I 2024-07-18 16:49:10,301] Trial 3 finished with value: 2271287290.887163 and parameters: {'alpha': 0.7324725546064613, 'epsilon_decay': 0.9870552260528945, 'alpha_decay': 0.0036062750683418495}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  2.271286290887163e-07  result:  2271287290.887163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1107.12it/s]\n",
      "[I 2024-07-18 16:49:19,338] Trial 4 finished with value: 14249358952.733189 and parameters: {'alpha': 0.5209812701387704, 'epsilon_decay': 0.9839775274305202, 'alpha_decay': 0.0013792505837090865}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.424935795273319e-06  result:  14249358952.733189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1102.43it/s]\n",
      "[I 2024-07-18 16:49:28,413] Trial 5 finished with value: 13776144304.15639 and parameters: {'alpha': 0.8651003363049525, 'epsilon_decay': 0.9594625070099438, 'alpha_decay': 0.009060346555406135}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.377614330415639e-06  result:  13776144304.15639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1100.40it/s]\n",
      "[I 2024-07-18 16:49:37,504] Trial 6 finished with value: 8271835848.88615 and parameters: {'alpha': 0.6541050153996902, 'epsilon_decay': 0.9742443537996618, 'alpha_decay': 0.006527965054017043}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.271834848886151e-07  result:  8271835848.88615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1092.22it/s]\n",
      "[I 2024-07-18 16:49:46,663] Trial 7 finished with value: 6909343682.073338 and parameters: {'alpha': 0.7854652494354548, 'epsilon_decay': 0.982427826053449, 'alpha_decay': 0.007194468341179152}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  6.909342682073338e-07  result:  6909343682.073338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1094.12it/s]\n",
      "[I 2024-07-18 16:49:55,805] Trial 8 finished with value: 2591215280.046504 and parameters: {'alpha': 0.7002435775526317, 'epsilon_decay': 0.9836375673375694, 'alpha_decay': 0.0026869466380097552}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  2.591214280046504e-07  result:  2591215280.046504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1090.97it/s]\n",
      "[I 2024-07-18 16:50:04,976] Trial 9 finished with value: 36921071491.19977 and parameters: {'alpha': 0.6738687075781138, 'epsilon_decay': 0.9823214788832735, 'alpha_decay': 0.0011783938500589822}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0]\n",
      "mse:  1.8460535245599883e-06  result:  36921071491.19977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1112.51it/s]\n",
      "[I 2024-07-18 16:50:13,980] Trial 10 finished with value: 4792587910.266373 and parameters: {'alpha': 0.5506299086898168, 'epsilon_decay': 0.9983503079197527, 'alpha_decay': 0.0035964603883481506}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  4.792586910266373e-07  result:  4792587910.266373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1076.16it/s]\n",
      "[I 2024-07-18 16:50:23,287] Trial 11 finished with value: 4725998214.099149 and parameters: {'alpha': 0.6210685300912411, 'epsilon_decay': 0.9732424976653498, 'alpha_decay': 0.004004355153826618}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  4.725997214099149e-07  result:  4725998214.099149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1120.74it/s]\n",
      "[I 2024-07-18 16:50:32,224] Trial 12 finished with value: 7760766502.398536 and parameters: {'alpha': 0.5915144412869034, 'epsilon_decay': 0.9988466001573317, 'alpha_decay': 0.003620195692479408}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  7.760765502398536e-07  result:  7760766502.398536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1114.02it/s]\n",
      "[I 2024-07-18 16:50:41,216] Trial 13 finished with value: 11514666719.259212 and parameters: {'alpha': 0.7279106369111319, 'epsilon_decay': 0.9691041053230696, 'alpha_decay': 0.004757680697272431}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.1514665719259212e-06  result:  11514666719.259212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1106.30it/s]\n",
      "[I 2024-07-18 16:50:50,269] Trial 14 finished with value: 5354217467.473397 and parameters: {'alpha': 0.7349033145405838, 'epsilon_decay': 0.99098695163233, 'alpha_decay': 0.002788207238842538}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  5.354216467473398e-07  result:  5354217467.473397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1108.76it/s]\n",
      "[I 2024-07-18 16:50:59,304] Trial 15 finished with value: 6901144268.511627 and parameters: {'alpha': 0.8865570417820525, 'epsilon_decay': 0.9777492070831925, 'alpha_decay': 0.005517396114096545}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  6.901143268511627e-07  result:  6901144268.511627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1110.34it/s]\n",
      "[I 2024-07-18 16:51:08,325] Trial 16 finished with value: 11964312344.620453 and parameters: {'alpha': 0.7975221225928371, 'epsilon_decay': 0.9500208930604233, 'alpha_decay': 0.005511134695113453}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.1964311344620452e-06  result:  11964312344.620453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1088.72it/s]\n",
      "[I 2024-07-18 16:51:17,527] Trial 17 finished with value: 6495474579.876577 and parameters: {'alpha': 0.6120776271541767, 'epsilon_decay': 0.9918147913755082, 'alpha_decay': 0.00249547344883495}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  6.495473579876577e-07  result:  6495474579.876577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1081.97it/s]\n",
      "[I 2024-07-18 16:51:26,785] Trial 18 finished with value: 10684529911.333252 and parameters: {'alpha': 0.7098165656021567, 'epsilon_decay': 0.97864791349699, 'alpha_decay': 0.00275462464326396}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.0684528911333252e-06  result:  10684529911.333252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1094.07it/s]\n",
      "[I 2024-07-18 16:51:35,940] Trial 19 finished with value: 7520992097.103741 and parameters: {'alpha': 0.8314788014470336, 'epsilon_decay': 0.967403579481462, 'alpha_decay': 0.004328942961505674}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  7.52099109710374e-07  result:  7520992097.103741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1082.64it/s]\n",
      "[I 2024-07-18 16:51:45,193] Trial 20 finished with value: 40544939270.09443 and parameters: {'alpha': 0.6488601154062383, 'epsilon_decay': 0.9885668205901488, 'alpha_decay': 0.002104195178814751}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0]\n",
      "mse:  2.0272469135047216e-06  result:  40544939270.09443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1099.12it/s]\n",
      "[I 2024-07-18 16:51:54,305] Trial 21 finished with value: 8171144845.2990265 and parameters: {'alpha': 0.6200038928242115, 'epsilon_decay': 0.971573951401323, 'alpha_decay': 0.0036346942076376587}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.171143845299026e-07  result:  8171144845.2990265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1100.52it/s]\n",
      "[I 2024-07-18 16:52:03,410] Trial 22 finished with value: 8365662665.1353035 and parameters: {'alpha': 0.5676458254833232, 'epsilon_decay': 0.9771071008875569, 'alpha_decay': 0.004575915094612373}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.365661665135304e-07  result:  8365662665.1353035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1099.44it/s]\n",
      "[I 2024-07-18 16:52:12,520] Trial 23 finished with value: 11750804890.401188 and parameters: {'alpha': 0.6905064708416484, 'epsilon_decay': 0.9943541302152051, 'alpha_decay': 0.003618259495071537}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.1750803890401188e-06  result:  11750804890.401188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1097.07it/s]\n",
      "[I 2024-07-18 16:52:21,650] Trial 24 finished with value: 4278320087.372349 and parameters: {'alpha': 0.7549091728490224, 'epsilon_decay': 0.9851339484103627, 'alpha_decay': 0.003029443778680305}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  4.278319087372349e-07  result:  4278320087.372349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1106.17it/s]\n",
      "[I 2024-07-18 16:52:30,708] Trial 25 finished with value: 10358225179.336845 and parameters: {'alpha': 0.7711750094725481, 'epsilon_decay': 0.9851785933981958, 'alpha_decay': 0.00296301088556353}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.0358224179336846e-06  result:  10358225179.336845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1082.83it/s]\n",
      "[I 2024-07-18 16:52:39,957] Trial 26 finished with value: 7760258970.558503 and parameters: {'alpha': 0.7617200141961702, 'epsilon_decay': 0.9806479546049621, 'alpha_decay': 0.005055599117309975}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  7.760257970558503e-07  result:  7760258970.558503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1063.50it/s]\n",
      "[I 2024-07-18 16:52:49,376] Trial 27 finished with value: 4480338410.711069 and parameters: {'alpha': 0.8382340164687628, 'epsilon_decay': 0.994426008941339, 'alpha_decay': 0.0018197860428761192}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  4.4803374107110687e-07  result:  4480338410.711069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1085.60it/s]\n",
      "[I 2024-07-18 16:52:58,603] Trial 28 finished with value: 4996982597.3471365 and parameters: {'alpha': 0.7005927343598028, 'epsilon_decay': 0.9874638557012154, 'alpha_decay': 0.006210542498180434}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  4.996981597347137e-07  result:  4996982597.3471365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1091.23it/s]\n",
      "[I 2024-07-18 16:53:07,784] Trial 29 finished with value: 4058221733.098794 and parameters: {'alpha': 0.653715426268994, 'epsilon_decay': 0.9855950320489446, 'alpha_decay': 0.0029905104043766923}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  4.058220733098794e-07  result:  4058221733.098794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1028.87it/s]\n",
      "[I 2024-07-18 16:53:17,519] Trial 30 finished with value: 8407646647.2655 and parameters: {'alpha': 0.6620522583944497, 'epsilon_decay': 0.9798969290622858, 'alpha_decay': 0.008702056980949276}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.4076456472655e-07  result:  8407646647.2655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 945.86it/s]\n",
      "[I 2024-07-18 16:53:28,107] Trial 31 finished with value: 12916623701.98352 and parameters: {'alpha': 0.7404108557662823, 'epsilon_decay': 0.9849540673234835, 'alpha_decay': 0.0031375421253280167}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.291662270198352e-06  result:  12916623701.98352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 940.28it/s]\n",
      "[I 2024-07-18 16:53:38,761] Trial 32 finished with value: 14167408952.834766 and parameters: {'alpha': 0.6875715352543957, 'epsilon_decay': 0.988537590685324, 'alpha_decay': 0.002269273543364112}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.4167407952834767e-06  result:  14167408952.834766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1013.34it/s]\n",
      "[I 2024-07-18 16:53:48,647] Trial 33 finished with value: 2867788773.655798 and parameters: {'alpha': 0.7589556373854621, 'epsilon_decay': 0.9941981214009106, 'alpha_decay': 0.0018194628515569894}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  2.867787773655798e-07  result:  2867788773.655798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 907.20it/s]\n",
      "[I 2024-07-18 16:53:59,687] Trial 34 finished with value: 9325784069.164402 and parameters: {'alpha': 0.7152398727003062, 'epsilon_decay': 0.991044903734177, 'alpha_decay': 0.0015620553400962373}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  9.325783069164401e-07  result:  9325784069.164402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 906.00it/s]\n",
      "[I 2024-07-18 16:54:10,745] Trial 35 finished with value: 12596938402.937798 and parameters: {'alpha': 0.6370461448420656, 'epsilon_decay': 0.9962743708970175, 'alpha_decay': 0.0019478714528914744}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.2596937402937798e-06  result:  12596938402.937798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:10<00:00, 912.45it/s]\n",
      "[I 2024-07-18 16:54:21,720] Trial 36 finished with value: 13044283001.97704 and parameters: {'alpha': 0.8127845950711262, 'epsilon_decay': 0.9939632473911367, 'alpha_decay': 0.0010376071124557995}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.304428200197704e-06  result:  13044283001.97704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 907.60it/s]\n",
      "[I 2024-07-18 16:54:32,756] Trial 37 finished with value: 25635803665.62742 and parameters: {'alpha': 0.6755060456229536, 'epsilon_decay': 0.9885411315488872, 'alpha_decay': 0.0015881920370942133}. Best is trial 3 with value: 2271287290.887163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 0]\n",
      "mse:  1.2817901332813708e-06  result:  25635803665.62742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:11<00:00, 885.01it/s]\n",
      "[I 2024-07-18 16:54:44,073] Trial 38 finished with value: 2267748228.5912256 and parameters: {'alpha': 0.7797582921883174, 'epsilon_decay': 0.9826151637519465, 'alpha_decay': 0.0024448201546440284}. Best is trial 38 with value: 2267748228.5912256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  2.2677472285912258e-07  result:  2267748228.5912256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1052.75it/s]\n",
      "[I 2024-07-18 16:54:53,588] Trial 39 finished with value: 11731860064.788298 and parameters: {'alpha': 0.7803786484675476, 'epsilon_decay': 0.9820510645601895, 'alpha_decay': 0.0024243322826659197}. Best is trial 38 with value: 2267748228.5912256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.1731859064788298e-06  result:  11731860064.788298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1037.66it/s]\n",
      "[I 2024-07-18 16:55:03,241] Trial 40 finished with value: 8647048122.058626 and parameters: {'alpha': 0.8519362336762333, 'epsilon_decay': 0.9756506191538832, 'alpha_decay': 0.004122385391079426}. Best is trial 38 with value: 2267748228.5912256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.647047122058627e-07  result:  8647048122.058626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1063.52it/s]\n",
      "[I 2024-07-18 16:55:12,663] Trial 41 finished with value: 9826156082.27533 and parameters: {'alpha': 0.8164947552350101, 'epsilon_decay': 0.9833247885359823, 'alpha_decay': 0.003388268335062188}. Best is trial 38 with value: 2267748228.5912256.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  9.82615508227533e-07  result:  9826156082.27533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1046.41it/s]\n",
      "[I 2024-07-18 16:55:22,235] Trial 42 finished with value: 2003475618.3572237 and parameters: {'alpha': 0.7476087889120938, 'epsilon_decay': 0.9868703659399009, 'alpha_decay': 0.0024299698986052446}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  2.0034746183572238e-07  result:  2003475618.3572237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1064.39it/s]\n",
      "[I 2024-07-18 16:55:31,645] Trial 43 finished with value: 9908398757.748167 and parameters: {'alpha': 0.7467360927705545, 'epsilon_decay': 0.9903438119025159, 'alpha_decay': 0.0013478393818409753}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  9.908397757748167e-07  result:  9908398757.748167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1035.15it/s]\n",
      "[I 2024-07-18 16:55:41,322] Trial 44 finished with value: 10545501162.638216 and parameters: {'alpha': 0.7197341939008113, 'epsilon_decay': 0.9934040204373626, 'alpha_decay': 0.0024381520610650335}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.0545500162638215e-06  result:  10545501162.638216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1063.82it/s]\n",
      "[I 2024-07-18 16:55:50,738] Trial 45 finished with value: 7996334674.0407505 and parameters: {'alpha': 0.7822684982816102, 'epsilon_decay': 0.9868136164447219, 'alpha_decay': 0.0019916432355377854}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  7.99633367404075e-07  result:  7996334674.0407505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1068.96it/s]\n",
      "[I 2024-07-18 16:56:00,108] Trial 46 finished with value: 8675607377.012733 and parameters: {'alpha': 0.7597830926045082, 'epsilon_decay': 0.9807538396611302, 'alpha_decay': 0.0038547280543320623}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.675606377012733e-07  result:  8675607377.012733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1074.00it/s]\n",
      "[I 2024-07-18 16:56:09,436] Trial 47 finished with value: 10954643947.170244 and parameters: {'alpha': 0.7972151310259208, 'epsilon_decay': 0.9966163950932045, 'alpha_decay': 0.0026695797857088865}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.0954642947170244e-06  result:  10954643947.170244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1072.55it/s]\n",
      "[I 2024-07-18 16:56:18,780] Trial 48 finished with value: 15564805025.77534 and parameters: {'alpha': 0.7355382995004838, 'epsilon_decay': 0.9831227246473869, 'alpha_decay': 0.009663692653706079}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.5564804025775339e-06  result:  15564805025.77534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1048.39it/s]\n",
      "[I 2024-07-18 16:56:28,335] Trial 49 finished with value: 8439252149.193464 and parameters: {'alpha': 0.8749467166295075, 'epsilon_decay': 0.9592512522197403, 'alpha_decay': 0.001689613097823854}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.439251149193464e-07  result:  8439252149.193464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1049.09it/s]\n",
      "[I 2024-07-18 16:56:37,882] Trial 50 finished with value: 9531178154.346401 and parameters: {'alpha': 0.6998388210742243, 'epsilon_decay': 0.9894683554329893, 'alpha_decay': 0.008272175428650958}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  9.5311771543464e-07  result:  9531178154.346401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1050.74it/s]\n",
      "[I 2024-07-18 16:56:47,418] Trial 51 finished with value: 13428116161.24931 and parameters: {'alpha': 0.6854556759225529, 'epsilon_decay': 0.9867032306858086, 'alpha_decay': 0.0032630231048743335}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.342811516124931e-06  result:  13428116161.24931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1055.55it/s]\n",
      "[I 2024-07-18 16:56:56,908] Trial 52 finished with value: 12011074706.702408 and parameters: {'alpha': 0.639725341073817, 'epsilon_decay': 0.9855678635535496, 'alpha_decay': 0.0022634201552738254}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  1.2011073706702407e-06  result:  12011074706.702408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1062.39it/s]\n",
      "[I 2024-07-18 16:57:06,337] Trial 53 finished with value: 5235548631.447423 and parameters: {'alpha': 0.7357050267289268, 'epsilon_decay': 0.9925741571508485, 'alpha_decay': 0.002845879504012068}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  5.235547631447423e-07  result:  5235548631.447423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1046.43it/s]\n",
      "[I 2024-07-18 16:57:15,913] Trial 54 finished with value: 8306377304.107523 and parameters: {'alpha': 0.6031618410611292, 'epsilon_decay': 0.978755961390728, 'alpha_decay': 0.0010008507796159158}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.306376304107523e-07  result:  8306377304.107523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1055.80it/s]\n",
      "[I 2024-07-18 16:57:25,401] Trial 55 finished with value: 6600820398.689061 and parameters: {'alpha': 0.7170918109408346, 'epsilon_decay': 0.9817880460088574, 'alpha_decay': 0.0033683776519774925}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  6.600819398689062e-07  result:  6600820398.689061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1054.31it/s]\n",
      "[I 2024-07-18 16:57:34,903] Trial 56 finished with value: 5592311071.204709 and parameters: {'alpha': 0.670824334288777, 'epsilon_decay': 0.9834491074860131, 'alpha_decay': 0.0051071299493256036}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  5.592310071204709e-07  result:  5592311071.204709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1057.93it/s]\n",
      "[I 2024-07-18 16:57:44,372] Trial 57 finished with value: 6447646868.246962 and parameters: {'alpha': 0.5153472411287797, 'epsilon_decay': 0.9757806847979248, 'alpha_decay': 0.0026666555908850306}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  6.447645868246962e-07  result:  6447646868.246962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1057.13it/s]\n",
      "[I 2024-07-18 16:57:53,851] Trial 58 finished with value: 36314549815.73017 and parameters: {'alpha': 0.7607319546798421, 'epsilon_decay': 0.9968057077576956, 'alpha_decay': 0.004217926262379087}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 1 0]\n",
      "mse:  1.8157274407865087e-06  result:  36314549815.73017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1048.51it/s]\n",
      "[I 2024-07-18 16:58:03,406] Trial 59 finished with value: 8730224985.20589 and parameters: {'alpha': 0.7923214846192019, 'epsilon_decay': 0.9869580462394187, 'alpha_decay': 0.0020325548311026036}. Best is trial 42 with value: 2003475618.3572237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 1 0]\n",
      "mse:  8.730223985205891e-07  result:  8730224985.20589\n",
      "Best hyperparameters:  {'alpha': 0.7476087889120938, 'epsilon_decay': 0.9868703659399009, 'alpha_decay': 0.0024299698986052446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1074.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1069.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1079.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1070.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1076.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 4 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1088.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1080.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1077.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 7 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:19<00:00, 1045.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 8 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:18<00:00, 1064.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9 FINAL OPTIMAL POLICY [0 0 0 0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Optuna study\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "optimal_alpha = study.best_params['alpha']\n",
    "optimal_epsilon_decay = study.best_params['epsilon_decay']\n",
    "optimal_alpha_decay = study.best_params['alpha_decay']\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    environment = environment\n",
    "    num_of_episodes = 20000\n",
    "    alpha = optimal_alpha\n",
    "    #gamma = 0\n",
    "    epsilon_decay = optimal_epsilon_decay\n",
    "    alpha_decay = optimal_alpha_decay\n",
    "    finding_parameters =  False\n",
    "    Q_tubular,_ = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\n",
    "    print(f\"\\n {i} FINAL OPTIMAL POLICY {np.argmax(Q_tubular,axis=1)}\")\n",
    "\n",
    "\n",
    "#Q_tubular = implement_Q_learning(environment, num_of_episodes, alpha, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Run the DQN for the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:01<00:00, 55.20it/s]\n"
     ]
    }
   ],
   "source": [
    "num_of_episodes = 10000\n",
    "NN_learning_rate = 0.01\n",
    "dql = stock_market_trading_DQN()\n",
    "optimal_network = dql.train_DQN(num_of_episodes,environment,gamma,NN_learning_rate)\n",
    "dql.test_DQN(10,environment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Optimal Policies Generated from diffrent algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 Optimal Policy\n",
      "State 0: Action 0\n",
      "State 1: Action 1\n",
      "State 2: Action 1\n",
      "State 3: Action 1\n",
      "State 4: Action 1\n",
      "State 5: Action 0\n",
      "State 6: Action 1\n",
      "State 7: Action 1\n",
      "State 8: Action 0\n",
      "State 9: Action 1\n",
      "State 10: Action 0\n",
      "State 11: Action 1\n",
      "State 12: Action 1\n",
      "State 13: Action 1\n",
      "State 14: Action 1\n",
      "State 15: Action 0\n",
      "State 16: Action 1\n",
      "State 17: Action 1\n",
      "State 18: Action 1\n",
      "State 19: Action 1\n",
      "State 20: Action 1\n",
      "State 21: Action 1\n",
      "State 22: Action 1\n",
      "State 23: Action 1\n",
      "\n",
      "Optimal Q =  [[0.03031047 0.02815876]\n",
      " [0.02638227 0.13133163]\n",
      " [0.04941365 0.05844365]\n",
      " [0.01009096 0.02041519]\n",
      " [0.01933133 0.10841806]\n",
      " [0.08369228 0.0101682 ]\n",
      " [0.02019804 0.0914925 ]\n",
      " [0.06360484 0.13300494]\n",
      " [0.06827976 0.03619543]\n",
      " [0.04675142 0.10075909]\n",
      " [0.01271144 0.01238558]\n",
      " [0.05064312 0.10624941]\n",
      " [0.00558377 0.09212666]\n",
      " [0.07747172 0.12413358]\n",
      " [0.00161899 0.12226172]\n",
      " [0.07114383 0.03561536]\n",
      " [0.05897908 0.07234668]\n",
      " [0.04094774 0.10078331]\n",
      " [0.02838915 0.076867  ]\n",
      " [0.00857494 0.04552349]\n",
      " [0.03768553 0.06137097]\n",
      " [0.04900211 0.07023331]\n",
      " [0.04102116 0.07171289]\n",
      " [0.03395023 0.06649623]]\n",
      "================================================================\n",
      "Phase 2 - DQN Optimal Policy\n",
      "00,1,[+0.03 +0.03]\n",
      "01,1,[+0.05 +0.07]\n",
      "02,0,[+0.06 +0.03]\n",
      "03,1,[-0.01 +0.01]\n",
      "\n",
      "04,1,[+0.05 +0.06]\n",
      "05,0,[+0.09 +0.02]\n",
      "06,1,[+0.03 +0.05]\n",
      "07,1,[+0.07 +0.07]\n",
      "\n",
      "08,0,[+0.08 +0.02]\n",
      "09,1,[+0.05 +0.05]\n",
      "10,0,[+0.02 +0.00]\n",
      "11,1,[+0.05 +0.06]\n",
      "\n",
      "12,1,[+0.02 +0.05]\n",
      "13,0,[+0.08 +0.05]\n",
      "14,1,[+0.05 +0.06]\n",
      "15,0,[+0.08 +0.02]\n",
      "\n",
      "16,0,[+0.06 +0.03]\n",
      "17,1,[+0.05 +0.06]\n",
      "18,0,[+0.04 +0.04]\n",
      "19,1,[+0.01 +0.02]\n",
      "\n",
      "20,0,[+0.04 +0.04]\n",
      "21,0,[+0.04 +0.04]\n",
      "22,0,[+0.04 +0.04]\n",
      "23,0,[+0.04 +0.04]\n",
      "\n",
      "================================================================\n",
      "\n",
      "Difference With NN\n",
      "[[ 0.02589574  0.02893985]\n",
      " [ 0.05127185  0.07035669]\n",
      " [ 0.05900951  0.0304405 ]\n",
      " [-0.00709187  0.01241054]\n",
      " [ 0.04813159  0.06101475]\n",
      " [ 0.09477926  0.02045076]\n",
      " [ 0.03173865  0.0464237 ]\n",
      " [ 0.06785353  0.07292898]\n",
      " [ 0.07958704  0.02469363]\n",
      " [ 0.04605915  0.05484944]\n",
      " [ 0.02245287  0.00468479]\n",
      " [ 0.04693208  0.0574463 ]\n",
      " [ 0.02254101  0.04918419]\n",
      " [ 0.08455123  0.05466224]\n",
      " [ 0.04840245  0.06182052]\n",
      " [ 0.08302356  0.02373388]\n",
      " [ 0.06308224  0.02930307]\n",
      " [ 0.04807255  0.06083909]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.01030549  0.02112795]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.03966977  0.03584168]]\n",
      "difference [[0.004414725793680158, -0.0007810905999239906], [-0.02488958658016814, 0.060974937856247], [-0.00959585312111206, 0.028003146499589057], [0.01718283170164258, 0.00800464607722818], [-0.02880025840334086, 0.04740330672671676], [-0.011086982461637501, -0.010282554160741676], [-0.011540608280792505, 0.04506880033315021], [-0.0042486864602547525, 0.06007596399039533], [-0.01130727949207691, 0.01150179965395804], [0.000692266226961967, 0.04590965208282173], [-0.009741431829398992, 0.007700791122763636], [0.003711045116518291, 0.048803112730448156], [-0.01695723605564086, 0.04294247585317268], [-0.007079505177956524, 0.06947134331745919], [-0.046783465581209244, 0.060441201296219924], [-0.011879722529648293, 0.0118814796998171], [-0.004103159745982118, 0.043043610352951595], [-0.007124805208642962, 0.039944226142635805], [-0.011280613047378096, 0.041025315913450905], [-0.0017305584422063931, 0.02439554568868116], [-0.001984240203642025, 0.025529284980825774], [0.009332344650486858, 0.03439162789604554], [0.0013513927024115385, 0.035871205827361446], [-0.005719539883460162, 0.030654540287958082]]\n",
      "Total Error: 0.0008927272074833766\n"
     ]
    }
   ],
   "source": [
    "# Phase 1 Optimal Policy\n",
    "print(\"Phase 1 Optimal Policy\")\n",
    "print_policy(P_opt1,len(environment))\n",
    "print(\"\\nOptimal Q = \",Q_opt)\n",
    "print(\"================================================================\")\n",
    "# Phase 2 - Tabular Q-Learning Optimal Policy\n",
    "# print(\"Phase 2 - Tubular Q-Learning Optimal Policy\")\n",
    "# print(np.argmax(Q_tubular,axis=1))\n",
    "# print(\"================================================================\")\n",
    "\n",
    "# Phase 2 - DQN Optimal Policy\n",
    "print(\"Phase 2 - DQN Optimal Policy\")\n",
    "Q_NN = dql.print_dqn(optimal_network)\n",
    "print(\"================================================================\")\n",
    "\n",
    "# Output difference\n",
    "# print(\"\\nDifference With Tabular\")\n",
    "# difference,total_error = calculate_difference_and_mse(Q_opt,Q_tubular)\n",
    "# print(f\"difference {difference}\\nTotal Error: {total_error}\")\n",
    "\n",
    "# Output difference\n",
    "print(\"\\nDifference With NN\")\n",
    "print(Q_NN)\n",
    "difference,total_error = calculate_difference_and_mse(Q_opt,Q_NN)\n",
    "print(f\"difference {difference}\\nTotal Error: {total_error}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
