{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 2: Stock Portfolio Optimization - Assignment 3**\n",
    " Athanasakis Evangelos 2019030118  \n",
    " Fragkogiannis Yiorgos 2019030039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Project 2: Stock Portfolio Optimization - Assignment 3**\n",
    "# Athanasakis Evangelos 2019030118 // Fragkogiannis Yiorgos 2019030039\n",
    "\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tkinter as tk #loads standard python GUI libraries\n",
    "import random\n",
    "from tkinter import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------__________________Environments___________________-------------------------------------------------------------------------\n",
    "# Generating environments\n",
    "\n",
    "\n",
    "# Create the three different environments\n",
    "# We are modeling this environment using 8 states in the format: {stock_currently_holding,state_of_stock_1,state_of_stock_2}\n",
    "\n",
    "action_keep = 0     # keep the same stock\n",
    "action_switch = 1   # switch to the other stock\n",
    "\n",
    "# This environment is used for the question 1 where we need to demonstrate that the optimal\n",
    "# policy is always to stay with the stock we already have invested\n",
    "fee = -0.9\n",
    "# r1H = 2*r2L\n",
    "# in this case r1.h=0.1 // r2.H= 0.05 // r1.L = -0.02 // r2.L = 0.01\n",
    "# we have used a large transaction fee so that the best policy will always be to keep using the same stock\n",
    "P1 = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "            (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "            (1/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "            (9/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (1/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (9/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# This environment implements the stocks environment from the midterm\n",
    "# It is used for the question 2 where we need to demonstrate that the optimal policy\n",
    "# for some of the states is to switch and in some others to stay\n",
    "fee = -0.01\n",
    "P2 = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05  + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "            (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "            (1/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "            (9/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# This environment implements the generic scenario of question 3 where for every stock\n",
    "# ri_H,ri_L are chosen uniformly in [-0.02, 0.1] and transition probabilities pi_HL, pi_LH\n",
    "# are equal to 0.1 for half the stocks and 0.5 for the other half.\n",
    "\n",
    "# Since every stock can have two price states, the number of total states in the MDP\n",
    "# we are creating will be = NumOfStoscks*2^numOfStocks\n",
    "\n",
    "\n",
    "def decimal_to_binary_array(decimal, length):\n",
    "\n",
    "    # Convert decimal to binary string (strip '0b' prefix)\n",
    "    binary_string = bin(decimal)[2:]\n",
    "\n",
    "    # Determine padding length\n",
    "    padding_length = max(0, length - len(binary_string))\n",
    "\n",
    "    # Pad binary string with leading zeros if needed\n",
    "    padded_binary_string = '0' * padding_length + binary_string\n",
    "\n",
    "    # Convert padded binary string to list of binary digits\n",
    "    binary_array = [int(bit) for bit in padded_binary_string]\n",
    "\n",
    "    return binary_array\n",
    "\n",
    "\n",
    "# Function that generates the environment of N stocks dynamically, with a transaction fee\n",
    "def generate_environment(N,fee):\n",
    "\n",
    "    states_for_each_stock = 2**N\n",
    "    total_states = N * states_for_each_stock\n",
    "    max_state_length = N\n",
    "\n",
    "    P = {}\n",
    "    pi = []\n",
    "    #Creating transition probabilities for the keep action\n",
    "    #of EACH stock\n",
    "    for i in range(0,N):\n",
    "        if(i < N/2):\n",
    "            # pi_HL = pi_LH = 0.1 | # pi_HH = pi_LL = 0.9\n",
    "            row = [0.9,0.1,0.1,0.9] #[LL,LH,HL,HH]\n",
    "        else:\n",
    "            # pi_HL = pi_LH = 0.5 | # pi_HH = pi_LL = 0.5\n",
    "            row = [0.5,0.5,0.5,0.5] #[LL,LH,HL,HH]\n",
    "        pi.append(row)\n",
    "\n",
    "    progress_bar = tqdm(range(0, total_states))\n",
    "    for i in progress_bar:\n",
    "        SubDictionary={}\n",
    "        action_Keep = []\n",
    "        action_Switch = []\n",
    "\n",
    "        # find what stock we are reffering to\n",
    "        # Stock ids start from 0\n",
    "        stock = i // states_for_each_stock\n",
    "\n",
    "        ##########################\n",
    "        # We define states of L and H with binary ids\n",
    "        # For example for 2 stocks this translation occurs:\n",
    "        # LL -> 0,0 -> 0\n",
    "        # LH -> 0,1 -> 1\n",
    "        # HL -> 1,0 -> 2\n",
    "        # HH -> 1,1 -> 3\n",
    "        # The binary ids are then translated to decimals so that\n",
    "        # we can use them in code\n",
    "        ##########################\n",
    "\n",
    "        current_state = i - stock * states_for_each_stock # find where this specific stock starts at the total_states environment\n",
    "                                                          # this is necessary to calculate the transition probabilities\n",
    "        # print(\"Current Stock: \",stock)\n",
    "        # print(\"Current State: \",current_state)\n",
    "        # Convert decimal to binary string\n",
    "        # Convert the binary string to a list of integers (0s and 1s)\n",
    "        curr_state_array = decimal_to_binary_array(current_state, max_state_length)\n",
    "        # We can now use the array to find if each stock is in high (1s) or low (0s) state\n",
    "        # So We now know that we are at state {x,L,L,H....,H} with x the number of current stock\n",
    "\n",
    "        #__Keep Stock ________________________________________________________________________________________________________________\n",
    "        # progress_1 = tqdm(range (stock*2**N, ((stock+1)*2**N)))\n",
    "        for j in range (stock*2**N, ((stock+1)*2**N)): # for every possible transition when keeping the same stock\n",
    "            state_to_trans = j - stock * states_for_each_stock          # value (H or L) of all of the stocks at the state we will transition to, in decimal form (0,1,2,3...)\n",
    "            trans_state_array = decimal_to_binary_array(state_to_trans, max_state_length) # convert to binary and take each bit separately (0 for L and 1 for H)\n",
    "            #print(trans_state_array)\n",
    "            transitionProb = 1\n",
    "            #print(f\"trans {transitionProb}\")\n",
    "            #print(\"len: \",len(trans_state_array))\n",
    "            for k in range(len(trans_state_array)):\n",
    "                stock_state_trans = trans_state_array[k]  # 0 or 1 // low or high\n",
    "                stock_state_current = curr_state_array[k] # 0 or 1 // low or high\n",
    "\n",
    "                if(stock_state_current == 0 and stock_state_trans == 0):       # Pi_LL\n",
    "                    transitionProb = transitionProb * pi[stock][0]\n",
    "                    #print(f\"{transitionProb} * {pi[stock][0]}\")\n",
    "                elif(stock_state_current == 0 and stock_state_trans == 1):     # pi_LH\n",
    "                    transitionProb = transitionProb * pi[stock][1]\n",
    "                elif(stock_state_current == 1 and stock_state_trans == 0):     # pi_HL\n",
    "                    transitionProb = transitionProb * pi[stock][2]\n",
    "                else:                                                          # pi_HH\n",
    "                    transitionProb = transitionProb * pi[stock][3]\n",
    "                #print(f\"{transitionProb}\")\n",
    "            nextState = j\n",
    "            #reward = random.uniform(-0.02, 20)\n",
    "            reward = random.uniform(-0.02, 0.1)\n",
    "            action_Keep.append((transitionProb,nextState,reward))\n",
    "            #print(\"\\nSWITCH\\n\")\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        #fee = 0\n",
    "        #__Switch Stock ________________________________________________________________________________________________________________\n",
    "        # progress_bar = tqdm(range (0, total_states))\n",
    "        for j in range (0, total_states): # for every possible transition \n",
    "            trans_stock = j // states_for_each_stock\n",
    "            \n",
    "            #trans_stock = j - stock * states_for_each_stock\n",
    "            if(trans_stock == stock):     # check if the transition stock is the same as the stock we start from\n",
    "                continue                  # we have already handle this situation above so we move on\n",
    "\n",
    "            #print(\"trans_stock \",trans_stock)\n",
    "            trans_state = j - trans_stock * states_for_each_stock\n",
    "            #print(\"Current: \",j)\n",
    "            #print(\"trans_state: \",trans_state)\n",
    "            trans_state_array = decimal_to_binary_array(trans_state, max_state_length)\n",
    "            #print(trans_state_array)\n",
    "            transitionProb = 1\n",
    "            #print(\"len \",len(trans_state_array))\n",
    "            for k in range(len(trans_state_array)):\n",
    "            #for k in range(len(trans_state)):\n",
    "                stock_state_trans = trans_state_array[k] # 0 or 1 // low or high\n",
    "                stock_state_current = curr_state_array[k] # 0 or 1 // low or high\n",
    "                #print(transitionProb)\n",
    "                if(stock_state_current == 0 and stock_state_trans == 0):       # Pi_LL\n",
    "                    transitionProb = transitionProb * pi[stock][0]\n",
    "                    #print(f\"{transitionProb} = {transitionProb}*{pi[stock][0]}\")\n",
    "                elif(stock_state_current == 0 and stock_state_trans == 1):     # pi_LH\n",
    "                    transitionProb = transitionProb * pi[stock][1]\n",
    "                    #print(f\"{transitionProb} = {transitionProb}*{pi[stock][1]}\")\n",
    "                elif(stock_state_current == 1 and stock_state_trans == 0):     # pi_HL\n",
    "                    transitionProb = transitionProb * pi[stock][2]\n",
    "                    #print(f\"{transitionProb} = {transitionProb}*{pi[stock][2]}\")\n",
    "                else:                                                          # pi_HH\n",
    "                    transitionProb = transitionProb * pi[stock][3]\n",
    "                    #print(f\"{transitionProb} = {transitionProb}*{pi[stock][3]}\")\n",
    "\n",
    "            nextState = j\n",
    "            #reward = random.uniform(-0.02, 20) - fee\n",
    "            reward = random.uniform(-0.02, 0.1) - fee\n",
    "            action_Switch.append((transitionProb*(1/(N-1)),nextState,reward))\n",
    "            \n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        SubDictionary[action_keep] = action_Keep\n",
    "        SubDictionary[action_switch] = action_Switch\n",
    "        P[i]=SubDictionary\n",
    "\n",
    "\n",
    "\n",
    "    return P\n",
    "\n",
    "#test = generate_environment(3,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1, Policy Evaluation/Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(pi, P, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
    "    #print(\"in policy EVALUATION\")\n",
    "    t = 0   #there's more elegant ways to do this\n",
    "    prev_V = np.zeros(len(P)) # use as \"cost-to-go\", i.e. for V(s')\n",
    "    while True:\n",
    "        V = np.zeros(len(P)) # current value function to be learnerd\n",
    "        for s in range(len(P)):  # do for every state\n",
    "            for prob, next_state, reward in P[s][pi(s)]:  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
    "                V[s] = np.int64(V[s] + prob * (reward + gamma * prev_V[next_state]))\n",
    "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one;     \n",
    "            break # if yes, finish loop\n",
    "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
    "        t += 1\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(V, P, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
    "    #print(\"in policy IMPROVEMENT\")\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64) #create a Q value array\n",
    "    for s in range(len(P)):        # for every state in the environment/model\n",
    "        for a in range(len(P[s])):  # and for every action in that state\n",
    "            for prob, next_state, reward in P[s][a]:  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve) \n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state])\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
    "    # lambda is a \"fancy\" way of creating a function without formally defining it (e.g. simply to return, as here...or to use internally in another function)\n",
    "    # you can implement this in a much simpler way, by using just a few more lines of code -- if this command is not clear, I suggest to try coding this yourself\n",
    "    \n",
    "    return new_pi,Q\n",
    "\n",
    "# policy iteration is simple, it will call alternatively policy evaluation then policy improvement, till the policy converges.\n",
    "\n",
    "def policy_iteration(P, gamma = 1.0, epsilon = 1e-10):\n",
    "    t = 0\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))     # start with random actions for each state  \n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
    "\n",
    "    while True:\n",
    "        old_pi = {s: pi(s) for s in range(len(P))}  #keep the old policy to compare with new\n",
    "        V = policy_evaluation(pi,P,gamma,epsilon)   #evaluate latest policy --> you receive its converged value function\n",
    "        pi,Q_table = policy_improvement(V,P,gamma)          #get a better policy using the value function of the previous one just calculated \n",
    "        \n",
    "        t += 1    \n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}: # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
    "            break\n",
    "    print('Converged after %d Policy Iterations' %t) #keep track of the number of (outer) iterations to converge\n",
    "    return V,pi,Q_table\n",
    "\n",
    "\n",
    "# Function to print policy\n",
    "def print_policy(policy, num_states=8):\n",
    "    for s in range(num_states):\n",
    "        print(f\"State {s}: Action {policy(s)}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions for Tubular Qlearning and DQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 23996.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 0.048805738070245526)"
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_difference_and_mse(q1, q2):\n",
    "    # Ensure the tables have the same dimensions\n",
    "    if len(q1) != len(q2) or any(len(row1) != len(row2) for row1, row2 in zip(q1, q2)):\n",
    "        raise ValueError(\"Both tables must have the same dimensions.\")\n",
    "    \n",
    "    result = []\n",
    "    total_squared_error = 0\n",
    "    num_elements = 0\n",
    "    \n",
    "    for row1, row2 in zip(q1, q2):\n",
    "        row_diff = []\n",
    "        for element1, element2 in zip(row1, row2):\n",
    "            diff = element1 - element2\n",
    "            row_diff.append(diff)\n",
    "            total_squared_error += diff ** 2\n",
    "            num_elements += 1\n",
    "        result.append(row_diff)\n",
    "    \n",
    "    mse = total_squared_error / num_elements\n",
    "    return result, mse\n",
    "\n",
    "\n",
    "def check_q_table_convergence(prev_Q, current_Q, epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Checks if the Q-table has converged.\n",
    "\n",
    "    Parameters:\n",
    "    - prev_Q (np.ndarray): Previous Q-table.\n",
    "    - current_Q (np.ndarray): Current Q-table.\n",
    "    - epsilon (float): Convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if Q-table has converged, False otherwise.\n",
    "    \"\"\"\n",
    "    if prev_Q is None:\n",
    "        return False  # Cannot determine convergence without a previous Q-table\n",
    "    \n",
    "    # Calculate the maximum absolute difference between corresponding Q-values\n",
    "    max_diff = np.max(np.abs(prev_Q - current_Q))\n",
    "    \n",
    "    # Check if the maximum difference is less than epsilon\n",
    "    if max_diff < epsilon:\n",
    "        return True  # Q-table has converged\n",
    "    \n",
    "    return False  # Q-table has not converged yet\n",
    "\n",
    "\n",
    "# This function is used to simulate the environments response\n",
    "# It gets as input the environment, the current state and the action that we have selected\n",
    "# and it returns the next state and the reward\n",
    "def get_response(environment, state, action):\n",
    "    P = environment\n",
    "    response = P[state][action] # get next states, transition probabilities and transaction rewards\n",
    "                                # based on the current state and the action we want to make   \n",
    "\n",
    "    #print(\"\\nResponse: \",response)\n",
    "\n",
    "    # we use random.choices to get a random next state based on the weighted probabilities of the next states\n",
    "    probabilities = []\n",
    "    avail_choices = range(len(P[state][action]))\n",
    "    # print(\"Choises = \",avail_choices)\n",
    "    # print(\"i in range \",len(P[state][action]))\n",
    "    for i in range(len(P[state][action])): \n",
    "        probabilities.append(response[i][0])\n",
    "    # envLength = len(P)\n",
    "    # print(envLength)\n",
    "    #probabilities = probabilities/(len(P)-1)\n",
    "    #print(\"\\nResponse i = \",response[i][0])\n",
    "    # print(\"\\nChoices: \",choices)\n",
    "    # print(\"Range i= \",len(P[state][action]))\n",
    "\n",
    "    # because depending on the action (keep or switch) the num of actions we can take is different\n",
    "    # hence, we check what the action we do is and declare the choices array accordingly\n",
    "        \n",
    "    # Make a random choice based on probabilities\n",
    "    # k=1: Specifies that we want to make a single random choice.\n",
    "    # [0] is used to extract the single element from that list\n",
    "    random_choice = random.choices(avail_choices, weights=probabilities, k=1)[0]\n",
    "    #print(random_choice)\n",
    "    next_state = response [random_choice][1] # get next state\n",
    "    reward = response [random_choice][2]     # get reward\n",
    "    # print(\"Reward \",reward)\n",
    "    # print(\"current: \",state)\n",
    "    # print(\"action: \",action)\n",
    "    # print(\"Next  \",next_state)\n",
    "    # print(\"prob\",probabilities)\n",
    "    # print(\"Sum\",np.sum(probabilities))\n",
    "    #print(P)\n",
    "\n",
    "    return next_state,reward\n",
    "\n",
    "test_p = generate_environment(3,0.01)\n",
    "#print(test_p)\n",
    "# #for i in range(10):\n",
    "get_response(test_p,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2\n",
    " Implementing Tubular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================================================\n",
    "#################### Q-Learning ################\n",
    "#===== Hyperparameters ===================\n",
    "# alpha -> Learning rate\n",
    "# gamma -> Discount factor\n",
    "# epsilon ->  # Exploration rate\n",
    "# epsilon_decay -> Decay rate for epsilon\n",
    "# min_epsilon -> Minimum epsilon value\n",
    "# num_episodes -> Number of episodes\n",
    "\n",
    "def implement_Q_learning(env, num_of_episodes, alpha, gamma, epsilon_decay=0.999, alpha_decay=0.001, finding_parameters=False):\n",
    "    num_of_states = len(env)\n",
    "    num_of_actions = len(env[0])\n",
    "    print(num_of_states,num_of_actions)\n",
    "   \n",
    "    Q = np.zeros((len(env),len(env[0])))\n",
    "    epsilon = 1.0                # Exploration rate0\n",
    "    #epsilon_decay = 0.99        # Decay rate for epsilon\n",
    "    min_epsilon = 0.1            # Minimum epsilon value\n",
    "    #alpha_decay = 0.01\n",
    "    initial_alpha = alpha\n",
    "    min_alpha = 0.001\n",
    "    #convergence_episode = float('inf')  # Initialize with a large number\n",
    "    conv_counter = 0\n",
    "\n",
    "    progress_bar = tqdm(range(num_of_episodes))\n",
    "\n",
    "    for episode in progress_bar: \n",
    "        prev_Q = np.copy(Q)\n",
    "        #current_state = random.choices(len(environment)-1, k=1)[0] \n",
    "        current_state = random.randint(0, len(env)-1) # select a random starting state\n",
    "        #print(\"Currene State: \",current_state)\n",
    "        for _ in range(100):      # do 100 steps do get a feel for what happens in the environment\n",
    "            # decide if we are going to explore or to exploit based on the epsilon value\n",
    "            # if random.uniform(0,1) < epsilon:\n",
    "            #     # Explore by picking a random action\n",
    "            #     action = random.choice([0,1])\n",
    "            #     #print(\"EXLORE ACTION: \",action)\n",
    "            # else:\n",
    "            #     action = np.argmax(Q[current_state])\n",
    "            #print(\"action\",action)\n",
    "            actions = [0,1]\n",
    "            action = random.choices([random.choices(actions, k=1)[0], np.argmax(Q[current_state])], weights=[epsilon, 1 - epsilon], k=1)[0]\n",
    "\n",
    "            next_state,reward = get_response(env, current_state, action)\n",
    "            #print(f\"Next State {next_state}\")\n",
    "            \n",
    "            # Q[current_state,action] = Q[current_state,action] + alpha * (\n",
    "            #     reward + gamma * np.max(Q[next_state]) - Q[current_state,action]\n",
    "            # )\n",
    "            # Q[current_state][action] = Q[current_state][action] + alpha * (\n",
    "            #     reward + gamma * np.max(Q[next_state][:]) - Q[current_state][action]\n",
    "            # )\n",
    "            Q[current_state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[current_state, action])\n",
    "            #print(f\"Q[{current_state},{action}] = {Q[current_state][action]}\")\n",
    "            # update the current state\n",
    "            current_state = next_state    \n",
    "        \n",
    "        # update the hyperparameters     \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        alpha = max(min_alpha, initial_alpha * np.exp(-alpha_decay * episode))\n",
    "\n",
    "        #print(\"epsilon: \",epsilon)\n",
    "        # if finding_parameters == True and check_q_table_convergence(prev_Q, Q, epsilon=0.00002):\n",
    "        #     conv_counter += 1\n",
    "        #     if conv_counter > 2:  # Adjust convergence criteria based on your problem\n",
    "        #         # convergence_episode = min(convergence_episode, episode)\n",
    "        #         convergence_episode = episode\n",
    "        #         # print(\"prev_Q:\", prev_Q)\n",
    "        #         # print(\"Q:\", Q)\n",
    "        #         print(\"convergence_episode = \",convergence_episode)\n",
    "        #         # print(np.argmax(Q,axis=1))\n",
    "        #         conv_counter = 0\n",
    "        #         break\n",
    "\n",
    "    # print(\"\\n\",Q)\n",
    "    convergence_episode = None\n",
    "    return Q, convergence_episode\n",
    "\n",
    "\n",
    "\n",
    "# environment = P2\n",
    "# alpha = 0.5\n",
    "# gamma = 0\n",
    "# V_opt1,P_opt1,Q_opt = policy_iteration(environment,gamma)\n",
    "\n",
    "\n",
    "# Define objective function for Optuna\n",
    "# Optuna tries to minimise the output of the objective function by modifying the hyperparameters of the tubular q learning algorithm\n",
    "# The output of the function will be the mse of the policy found at convergence summed up with the number of steps it took to converge\n",
    "# Because finding a correct policy is more important then the number of steps, mse is (weighted) multiplied with 10^14 (so that it has greater impact on \n",
    "# the output of the objective function)\n",
    "#def objective(trial):    \n",
    "    #environment = P2  # Define your environment here\n",
    "    num_of_episodes = 100000  # Adjust as needed\n",
    "    alpha = trial.suggest_float('alpha', 0.5, 0.9, log=True)\n",
    "    gamma = 0\n",
    "    epsilon_decay = trial.suggest_float('epsilon_decay', 0.95, 0.999)\n",
    "    alpha_decay = trial.suggest_float('alpha_decay', 0.001, 0.01)\n",
    "    finding_parameters = True    \n",
    "    Q, convergence_episode = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\n",
    "    print(np.argmax(Q,axis=1))\n",
    "    \n",
    "    # Return the inverse of convergence episode (maximize speed)\n",
    "    convergence_episode = convergence_episode if convergence_episode != float('inf') else 10000\n",
    "    r, mse = calculate_difference_and_mse(Q_opt, Q)   \n",
    "    difference_count = sum(1 for x, y in zip(np.argmax(Q_opt,axis=1), np.argmax(Q,axis=1)) if x != y)\n",
    "    result = mse * 10000000000000000 * (difference_count+1) + convergence_episode/10 \n",
    "    print(\"mse: \",mse,\" result: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def count_tables_differences(table1, table2):\n",
    "    if len(table1) != len(table2):\n",
    "        raise ValueError(\"Both tables must have the same length.\")\n",
    "    \n",
    "    difference_count = sum(1 for x, y in zip(table1, table2) if x != y)\n",
    "    return difference_count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Deep Q-Learning Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################____TASK3____########################################\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Class That Implements Our Deep Q-Network\n",
    "class stock_market_trading_DQN():    \n",
    "    # HyperParameters\n",
    "    alpha = 0.001              # Learning rate\n",
    "    gamma = 0              # Discount Factor\n",
    "    synching_period = 100    # After this many batches we synch the target nn with the policy nn\n",
    "    replay_buffer_size = 10000 # Size of replay buffer\n",
    "    min_batch_size = 64      # Size of each batch\n",
    "    #optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "\n",
    "    # Define Huber as our loss function\n",
    "    # loss_func = nn.SmoothL1Loss()\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    ACTIONS = [0,1]\n",
    "    num_actions = 2\n",
    "    \n",
    "    # Encode the input state \n",
    "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states)\n",
    "        input_tensor[state] = 1\n",
    "        return input_tensor\n",
    "            \n",
    "    # This method is responsible to train our network based on a number of 'episodes'\n",
    "    def train_DQN(self, episodes,environment,gamma,lr):\n",
    "        P = environment\n",
    "        num_of_states = len(P)\n",
    "        num_of_actions = len(P[0])\n",
    "        \n",
    "        epsilon = 1 # Exploration rate\n",
    "        self.gamma = gamma\n",
    "        self.alpha = lr\n",
    "        memory_buffer = ReplayMemory(self.replay_buffer_size)\n",
    "        #memory_buffer = [[] for _ in range(self.replay_buffer_size)] \n",
    "        \n",
    "        #memory_buffer[i % 1000] = [0,1,2,3]\n",
    "        \n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        # We create a NN with num of input nodes equal to the num of the total states \n",
    "        # The num of output layer nodes is equal to the num of the total actions\n",
    "        # The hidden layer's num of nodes is equal to the num of states -> this is adjustable\n",
    "        policy_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions)\n",
    "        target_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions)\n",
    "\n",
    "        # initialize the 2 networks to be the same \n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # print('Policy (random, before training):')\n",
    "        # self.print_dqn(policy_dqn)\n",
    "        # print('===============================================================')\n",
    "        # print('===============================================================')\n",
    "\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        # self.optimizer = torch.optim.RMSprop(policy_dqn.parameters(), lr=self.alpha, alpha=0.99, \n",
    "        #                                      eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.alpha)\n",
    "        # optimizer = SGD([parameter], lr=0.1)\n",
    "        \n",
    "        # keep track of the reward at each round \n",
    "        reward_tracking = np.zeros(episodes)\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_tracking = []\n",
    "        synch_counter = 0 # which step we are on \n",
    "        \n",
    "        progress_bar = tqdm(range(episodes))\n",
    "        for i in progress_bar:\n",
    "            current_state = random.randint(0, len(P)-1) # select a random starting state\n",
    "        \n",
    "            for _ in range(100):      # do 100 steps do get a feel for what happens in the environment\n",
    "                # decide if we are going to explore or to exploit based on the epsilon value\n",
    "                # if random.uniform(0,1) < epsilon:\n",
    "                if random.random() < epsilon:\n",
    "                    #action = np.random.binomial(1,0.5)     # Explore by picking a random action\n",
    "                    action = random.choice([0,1])\n",
    "                else:\n",
    "                     # From the output layer, choose the node output (action) with the maximum value\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(current_state, num_of_states)).argmax().item()\n",
    "                    \n",
    "                # get the response from the environment\n",
    "                next_state,reward = get_response(P, current_state, action)\n",
    "                # reward_tracking[i] = reward\n",
    "                \n",
    "                # Store the environments response into our memory        \n",
    "                # memory_buffer[step % 1000] = [current_state, action, next_state, reward]\n",
    "                memory_buffer.append((current_state, action, next_state, reward)) \n",
    "            \n",
    "                # update the next state\n",
    "                current_state = next_state    \n",
    "            \n",
    "                # Increment step counter\n",
    "                synch_counter += 1\n",
    "            \n",
    "            # Perform the optimization\n",
    "            if(len(memory_buffer) > self.min_batch_size):\n",
    "\n",
    "                #mini_batch = self.sample_mem_buffer(memory_buffer, self.min_batch_size)\n",
    "                mini_batch = memory_buffer.sample(self.min_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                #epsilon = max(epsilon * 0.99, 0.1)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                ### CHECK\n",
    "                # if (step % self.synching_period) == 0:\n",
    "                if synch_counter > self.synching_period :\n",
    "                # if (synch_counter  self.synching_period): \n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    synch_counter = 0\n",
    "\n",
    "        # return the optimal policy\n",
    "        #return policy_dqn.state_dict()\n",
    "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
    "        return policy_dqn\n",
    "                \n",
    "    def optimize(self,mini_batch, policy_dqn, target_dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = policy_dqn.fc1.in_features\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward in mini_batch:\n",
    "            # Calculate target q value \n",
    "            # We disable the gradient tracking for memory optimization\n",
    "            with torch.no_grad():\n",
    "                # Here we get the optimal output we SHOULD have gotten according to the target NN\n",
    "                target = torch.FloatTensor(\n",
    "                    # For DQNs the target NNs parameters are modified according to the equation\n",
    "                    # Q[state,action] = reward + γ *max{Q[next_state]}\n",
    "                    reward + self.gamma * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
    "                )\n",
    "                    \n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state, num_states)) \n",
    "\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # calculate the loss for all the batch  \n",
    "        loss = self.loss_func(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model by running back-propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Test function\n",
    "    def test_DQN(self, episodes,environment):\n",
    "        # Create FrozenLake instance\n",
    "        P = environment\n",
    "        num_of_states = len(P)\n",
    "        num_of_actions = len(P[0])\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions) \n",
    " \n",
    "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        # print('Policy (trained):')\n",
    "        # self.print_dqn(policy_dqn)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            current_state = random.randint(0, num_of_states-1)\n",
    "\n",
    "            for _ in range(100):\n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(current_state, num_of_states)).argmax().item()\n",
    "                # Execute action\n",
    "                current_state,reward = get_response(P, current_state, action)\n",
    "\n",
    "        \n",
    "        \n",
    "    def print_dqn(self, dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = dqn.fc1.in_features\n",
    "        Q_table = np.zeros((num_states, self.num_actions))\n",
    "\n",
    "        # Loop each state and print policy to console\n",
    "        for s in range(num_states):\n",
    "\n",
    "            q_values_element = dqn(self.state_to_dqn_input(s, num_states)).tolist()\n",
    "            Q_table[s] = q_values_element\n",
    "            \n",
    "            #  Format q values for printing\n",
    "            q_values = ''\n",
    "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
    "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
    "            q_values=q_values.rstrip()              # Remove space at the end\n",
    "            #\n",
    "\n",
    "            # Map the best action\n",
    "            best_action = dqn(self.state_to_dqn_input(s, num_states)).argmax()\n",
    "\n",
    "            # Print policy in the format of: state, action, q values\n",
    "            # The printed layout matches the FrozenLake map.\n",
    "            print(f'{s:02},{best_action},[{q_values}]', end='\\n')         \n",
    "            if (s+1)%4==0:\n",
    "                print() # Print a newline every 4 states\n",
    "            \n",
    "        #Q_table_transposed = [list(row) for row in zip(*Q_table)]\n",
    "        return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna study for DQN (hyperparameter auto-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     # Hyperparameters to tune\n",
    "#     alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
    "#     # gamma = trial.suggest_uniform('gamma', 0.8, 1.0)\n",
    "#     gamma = 0\n",
    "#     synching_period = trial.suggest_int('synching_period', 50, 500)\n",
    "#     replay_buffer_size = trial.suggest_int('replay_buffer_size', 1000, 20000)\n",
    "#     min_batch_size = trial.suggest_int('min_batch_size', 32, 256)\n",
    "    \n",
    "#     # Create an instance of the stock_market_trading_DQN class\n",
    "#     dqn_agent = stock_market_trading_DQN()\n",
    "    \n",
    "#     # Dummy environment\n",
    "#     num_states = 16\n",
    "#     num_actions = 2\n",
    "#     P = np.random.rand(num_states, num_actions)\n",
    "    \n",
    "#     # Train the DQN with the given hyperparameters\n",
    "#     trained_policy = dqn_agent.train_DQN(\n",
    "#         episodes=500,\n",
    "#         environment=P,\n",
    "#         gamma=gamma,\n",
    "#         lr=alpha\n",
    "#     )\n",
    "    \n",
    "#     # Test the trained DQN\n",
    "#     total_reward = 0\n",
    "#     for _ in range(10):  # Run multiple episodes for evaluation\n",
    "#         current_state = random.randint(0, len(P) - 1)\n",
    "#         for _ in range(100):  # Steps in an episode\n",
    "#             with torch.no_grad():\n",
    "#                 action = trained_policy(dqn_agent.state_to_dqn_input(current_state, num_states)).argmax().item()\n",
    "#             current_state, reward = get_response(P, current_state, action)\n",
    "#             total_reward += reward\n",
    "    \n",
    "#     # Return the total reward as the objective to maximize\n",
    "#     return total_reward\n",
    "\n",
    "# # Create a study and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=100)\n",
    "\n",
    "# print('Best hyperparameters: ', study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 9072.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#environment = P2\n",
    "environment = generate_environment(4,0.01)\n",
    "#print(environment)\n",
    "gamma = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Optimal Policy (policy Iteration -> Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 Policy Iterations\n",
      "[0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "V_opt1,P_opt1,Q_opt = policy_iteration(environment,gamma)\n",
    "print(np.argmax(Q_opt,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Tubular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:20<00:00, 974.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "\n",
      "FINAL OPTIMAL POLICY [0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "Qopt =  [[ 0.01942297 -0.00050326]\n",
      " [ 0.01573466  0.0313923 ]\n",
      " [ 0.00929088  0.01517256]\n",
      " [ 0.01576859  0.02413415]\n",
      " [ 0.0300999   0.02606818]\n",
      " [ 0.02435853  0.04020411]\n",
      " [ 0.05990849  0.03997383]\n",
      " [ 0.04130693  0.04715185]\n",
      " [ 0.07470624  0.04119502]\n",
      " [ 0.0461816   0.04071322]\n",
      " [ 0.05822092  0.04784855]\n",
      " [ 0.06739863  0.03112367]\n",
      " [ 0.04776802  0.04186393]\n",
      " [ 0.0052796   0.05686487]\n",
      " [ 0.03069875  0.04112759]\n",
      " [ 0.04928658  0.04009006]\n",
      " [ 0.03313555  0.02250945]\n",
      " [ 0.00483296  0.034345  ]\n",
      " [ 0.03613388  0.00652115]\n",
      " [ 0.05452196  0.02381026]\n",
      " [ 0.07187305  0.02380688]\n",
      " [ 0.00821421  0.0129259 ]\n",
      " [ 0.07945874  0.0569656 ]\n",
      " [-0.00441221  0.05026472]\n",
      " [ 0.02826453  0.04125478]\n",
      " [ 0.00870836  0.00702023]\n",
      " [ 0.0393882   0.04955945]\n",
      " [ 0.03249386  0.04793444]\n",
      " [ 0.01740613  0.05162493]\n",
      " [ 0.02550755  0.01308746]\n",
      " [ 0.03033769  0.01574399]\n",
      " [-0.00139066  0.04783794]\n",
      " [ 0.03510709  0.03423882]\n",
      " [ 0.04092267  0.02395458]\n",
      " [ 0.04359492  0.02336262]\n",
      " [ 0.01685981  0.03835484]\n",
      " [ 0.05585426  0.02918094]\n",
      " [ 0.052234    0.03023626]\n",
      " [ 0.04391719  0.02288264]\n",
      " [ 0.04432406  0.03206444]\n",
      " [ 0.05751725  0.02498351]\n",
      " [ 0.05232267  0.03035495]\n",
      " [ 0.02407081  0.02368964]\n",
      " [ 0.04644609  0.03002324]\n",
      " [ 0.04695574  0.03372223]\n",
      " [ 0.0487909   0.02841567]\n",
      " [ 0.05269832  0.02646069]\n",
      " [ 0.05052285  0.03221689]\n",
      " [ 0.03454872  0.03428627]\n",
      " [ 0.04036826  0.02748139]\n",
      " [ 0.05365054  0.02383128]\n",
      " [ 0.03807274  0.02601674]\n",
      " [ 0.04619247  0.03547629]\n",
      " [ 0.04571427  0.02719129]\n",
      " [ 0.04164192  0.02701827]\n",
      " [ 0.04288147  0.0306359 ]\n",
      " [ 0.04504471  0.03042463]\n",
      " [ 0.03670747  0.03918305]\n",
      " [ 0.0463439   0.03268167]\n",
      " [ 0.04013352  0.02823438]\n",
      " [ 0.04869832  0.0257745 ]\n",
      " [ 0.04537759  0.0267079 ]\n",
      " [ 0.03463009  0.02710515]\n",
      " [ 0.05488273  0.03619169]]\n",
      "\n",
      "Qtabular =  [[ 0.01934789  0.00093038]\n",
      " [ 0.01502034  0.03167852]\n",
      " [ 0.00786894  0.01505018]\n",
      " [ 0.01509702  0.0242123 ]\n",
      " [ 0.02949802  0.02723719]\n",
      " [ 0.02411549  0.03964823]\n",
      " [ 0.05972255  0.04122332]\n",
      " [ 0.04134178  0.04651839]\n",
      " [ 0.0740923   0.04226552]\n",
      " [ 0.04634698  0.04037293]\n",
      " [ 0.05661606  0.04792587]\n",
      " [ 0.06777604  0.03213127]\n",
      " [ 0.04812721  0.04400187]\n",
      " [ 0.00644805  0.0565144 ]\n",
      " [ 0.03323299  0.04139677]\n",
      " [ 0.04919674  0.03999564]\n",
      " [ 0.03234819  0.02098107]\n",
      " [ 0.00537694  0.03391902]\n",
      " [ 0.03528366  0.00665481]\n",
      " [ 0.05539537  0.02232454]\n",
      " [ 0.07135971  0.02395681]\n",
      " [ 0.00479732  0.01466003]\n",
      " [ 0.08030444  0.05791022]\n",
      " [-0.00340349  0.05019922]\n",
      " [ 0.02546979  0.04154697]\n",
      " [ 0.00822327  0.00513838]\n",
      " [ 0.03570875  0.04839194]\n",
      " [ 0.03129953  0.0483736 ]\n",
      " [ 0.01766222  0.05142705]\n",
      " [ 0.02543111  0.01315206]\n",
      " [ 0.02990271  0.01638869]\n",
      " [-0.00192439  0.04773122]\n",
      " [ 0.03343949  0.03437761]\n",
      " [ 0.04075947  0.02409873]\n",
      " [ 0.04360156  0.02362649]\n",
      " [ 0.01565476  0.03894577]\n",
      " [ 0.05465767  0.03001979]\n",
      " [ 0.05231952  0.03043339]\n",
      " [ 0.04461735  0.02297221]\n",
      " [ 0.04382933  0.03214796]\n",
      " [ 0.05788328  0.02592004]\n",
      " [ 0.05351575  0.03100335]\n",
      " [ 0.02525835  0.02213562]\n",
      " [ 0.04674399  0.02977805]\n",
      " [ 0.04747327  0.03410054]\n",
      " [ 0.04761945  0.02723836]\n",
      " [ 0.05209683  0.02719658]\n",
      " [ 0.05159701  0.03107587]\n",
      " [ 0.03431865  0.03325665]\n",
      " [ 0.04059334  0.02663151]\n",
      " [ 0.05287364  0.02379864]\n",
      " [ 0.03867119  0.02503786]\n",
      " [ 0.04530767  0.03692449]\n",
      " [ 0.0454328   0.02688502]\n",
      " [ 0.04222775  0.02772619]\n",
      " [ 0.04238622  0.03025127]\n",
      " [ 0.04422113  0.02973537]\n",
      " [ 0.0376121   0.03876735]\n",
      " [ 0.04509679  0.03367588]\n",
      " [ 0.03950323  0.02812179]\n",
      " [ 0.04907698  0.0257944 ]\n",
      " [ 0.04613829  0.02625053]\n",
      " [ 0.03563047  0.02779713]\n",
      " [ 0.05460643  0.03590589]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Optuna study\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=40)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# optimal_alpha = study.best_params['alpha']\n",
    "# optimal_epsilon_decay = study.best_params['epsilon_decay']\n",
    "# optimal_alpha_decay = study.best_params['alpha_decay']\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     environment = environment\n",
    "#     num_of_episodes = 50000\n",
    "#     alpha = optimal_alpha\n",
    "#     #gamma = 0\n",
    "#     epsilon_decay = optimal_epsilon_decay\n",
    "#     alpha_decay = optimal_alpha_decay\n",
    "#     finding_parameters =  False\n",
    "#     Q_tubular,_ = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\n",
    "#     print(f\"\\n {i} FINAL OPTIMAL POLICY {np.argmax(Q_tubular,axis=1)}\")\n",
    "\n",
    "\n",
    "Q_tubular,_ = implement_Q_learning(environment, num_of_episodes=20000, alpha=0.5, gamma=gamma)\n",
    "print(np.argmax(Q_opt,axis=1))\n",
    "print(f\"\\nFINAL OPTIMAL POLICY {np.argmax(Q_tubular,axis=1)}\")\n",
    "print(\"Qopt = \",Q_opt)\n",
    "print(\"\\nQtabular = \",Q_tubular)\n",
    "# diff = count_tables_differences(Q_opt,Q_tubular)\n",
    "# print(\"DIfference: \",diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Run the DQN for the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2050/10000 [01:06<04:04, 32.50it/s]"
     ]
    }
   ],
   "source": [
    "num_of_episodes = 10000\n",
    "NN_learning_rate = 0.01\n",
    "dql = stock_market_trading_DQN()\n",
    "optimal_network = dql.train_DQN(num_of_episodes,environment,gamma,NN_learning_rate)\n",
    "dql.test_DQN(10,environment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Optimal Policies Generated from diffrent algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 Optimal Policy\n",
      "State 0: Action 0\n",
      "State 1: Action 1\n",
      "State 2: Action 0\n",
      "State 3: Action 0\n",
      "State 4: Action 0\n",
      "State 5: Action 0\n",
      "State 6: Action 0\n",
      "State 7: Action 0\n",
      "State 8: Action 1\n",
      "State 9: Action 0\n",
      "State 10: Action 0\n",
      "State 11: Action 0\n",
      "State 12: Action 0\n",
      "State 13: Action 0\n",
      "State 14: Action 0\n",
      "State 15: Action 0\n",
      "State 16: Action 0\n",
      "State 17: Action 0\n",
      "State 18: Action 1\n",
      "State 19: Action 1\n",
      "State 20: Action 0\n",
      "State 21: Action 1\n",
      "State 22: Action 1\n",
      "State 23: Action 1\n",
      "State 24: Action 0\n",
      "State 25: Action 0\n",
      "State 26: Action 1\n",
      "State 27: Action 0\n",
      "State 28: Action 1\n",
      "State 29: Action 0\n",
      "State 30: Action 1\n",
      "State 31: Action 1\n",
      "State 32: Action 1\n",
      "State 33: Action 0\n",
      "State 34: Action 0\n",
      "State 35: Action 1\n",
      "State 36: Action 0\n",
      "State 37: Action 0\n",
      "State 38: Action 0\n",
      "State 39: Action 0\n",
      "State 40: Action 0\n",
      "State 41: Action 0\n",
      "State 42: Action 0\n",
      "State 43: Action 0\n",
      "State 44: Action 0\n",
      "State 45: Action 0\n",
      "State 46: Action 0\n",
      "State 47: Action 0\n",
      "State 48: Action 1\n",
      "State 49: Action 0\n",
      "State 50: Action 0\n",
      "State 51: Action 0\n",
      "State 52: Action 0\n",
      "State 53: Action 0\n",
      "State 54: Action 0\n",
      "State 55: Action 0\n",
      "State 56: Action 0\n",
      "State 57: Action 0\n",
      "State 58: Action 0\n",
      "State 59: Action 1\n",
      "State 60: Action 0\n",
      "State 61: Action 0\n",
      "State 62: Action 0\n",
      "State 63: Action 0\n",
      "\n",
      "Optimal Q =  [[0.06006104 0.03231367]\n",
      " [0.01213174 0.01749841]\n",
      " [0.04294887 0.01813564]\n",
      " [0.06694194 0.03938772]\n",
      " [0.07544221 0.03188979]\n",
      " [0.05270711 0.02294812]\n",
      " [0.06258706 0.03250943]\n",
      " [0.05683976 0.02326668]\n",
      " [0.01776808 0.04258229]\n",
      " [0.0561957  0.03792793]\n",
      " [0.04980682 0.01394365]\n",
      " [0.0612891  0.05985654]\n",
      " [0.0354043  0.01892757]\n",
      " [0.05916049 0.01511948]\n",
      " [0.03169853 0.0265217 ]\n",
      " [0.02839269 0.02813512]\n",
      " [0.06565217 0.03669038]\n",
      " [0.01459309 0.00988594]\n",
      " [0.02862712 0.04148608]\n",
      " [0.01706875 0.04125367]\n",
      " [0.07347436 0.05408193]\n",
      " [0.03354832 0.05766974]\n",
      " [0.02832928 0.03943408]\n",
      " [0.01935913 0.024458  ]\n",
      " [0.06350153 0.01362153]\n",
      " [0.08116874 0.02269794]\n",
      " [0.01925488 0.02607741]\n",
      " [0.04056708 0.04055827]\n",
      " [0.01500986 0.05622584]\n",
      " [0.04936113 0.02565798]\n",
      " [0.01691972 0.0264702 ]\n",
      " [0.01432558 0.03012116]\n",
      " [0.02566224 0.03704237]\n",
      " [0.03062626 0.02295231]\n",
      " [0.06246422 0.01766388]\n",
      " [0.02405817 0.03362642]\n",
      " [0.0334597  0.03203566]\n",
      " [0.03751832 0.03409844]\n",
      " [0.05277399 0.02170622]\n",
      " [0.03143846 0.02939796]\n",
      " [0.05405756 0.03177182]\n",
      " [0.04566492 0.0245114 ]\n",
      " [0.03570042 0.02933296]\n",
      " [0.03299225 0.02844127]\n",
      " [0.0374936  0.02883498]\n",
      " [0.03186141 0.02927476]\n",
      " [0.01990337 0.01879286]\n",
      " [0.0593208  0.02329183]\n",
      " [0.02196512 0.03731138]\n",
      " [0.06228565 0.0285412 ]\n",
      " [0.04644259 0.02923911]\n",
      " [0.04410807 0.02371815]\n",
      " [0.04814968 0.03738996]\n",
      " [0.03744589 0.03428523]\n",
      " [0.05037896 0.03139468]\n",
      " [0.05763928 0.03175088]\n",
      " [0.04518567 0.02583966]\n",
      " [0.03995079 0.02752798]\n",
      " [0.05084435 0.02658884]\n",
      " [0.03990763 0.04009155]\n",
      " [0.05544369 0.02456602]\n",
      " [0.03988988 0.03692143]\n",
      " [0.04565263 0.03036929]\n",
      " [0.03588707 0.02955893]]\n",
      "================================================================\n",
      "Phase 2 - DQN Optimal Policy\n",
      "00,0,[+0.06 +0.04]\n",
      "01,1,[+0.03 +0.03]\n",
      "02,0,[+0.03 +0.02]\n",
      "03,0,[+0.06 +0.04]\n",
      "\n",
      "04,0,[+0.07 +0.03]\n",
      "05,0,[+0.05 +0.02]\n",
      "06,0,[+0.05 +0.03]\n",
      "07,0,[+0.05 +0.04]\n",
      "\n",
      "08,1,[+0.03 +0.03]\n",
      "09,0,[+0.05 +0.04]\n",
      "10,0,[+0.04 +0.02]\n",
      "11,0,[+0.06 +0.04]\n",
      "\n",
      "12,1,[+0.03 +0.03]\n",
      "13,0,[+0.05 +0.01]\n",
      "14,1,[+0.03 +0.03]\n",
      "15,1,[+0.03 +0.03]\n",
      "\n",
      "16,0,[+0.06 +0.04]\n",
      "17,1,[+0.03 +0.03]\n",
      "18,1,[+0.03 +0.03]\n",
      "19,1,[+0.03 +0.03]\n",
      "\n",
      "20,0,[+0.07 +0.05]\n",
      "21,1,[+0.03 +0.03]\n",
      "22,1,[+0.03 +0.03]\n",
      "23,1,[+0.03 +0.03]\n",
      "\n",
      "24,0,[+0.05 +0.01]\n",
      "25,0,[+0.08 +0.03]\n",
      "26,1,[+0.03 +0.03]\n",
      "27,1,[+0.03 +0.03]\n",
      "\n",
      "28,1,[+0.01 +0.04]\n",
      "29,0,[+0.04 +0.02]\n",
      "30,1,[+0.03 +0.03]\n",
      "31,1,[+0.03 +0.03]\n",
      "\n",
      "32,1,[+0.03 +0.03]\n",
      "33,1,[+0.03 +0.03]\n",
      "34,0,[+0.04 +0.02]\n",
      "35,1,[+0.03 +0.03]\n",
      "\n",
      "36,1,[+0.03 +0.03]\n",
      "37,1,[+0.03 +0.03]\n",
      "38,0,[+0.05 +0.02]\n",
      "39,1,[+0.03 +0.03]\n",
      "\n",
      "40,0,[+0.05 +0.04]\n",
      "41,0,[+0.03 +0.03]\n",
      "42,1,[+0.03 +0.03]\n",
      "43,1,[+0.03 +0.03]\n",
      "\n",
      "44,1,[+0.03 +0.03]\n",
      "45,1,[+0.03 +0.03]\n",
      "46,1,[+0.03 +0.03]\n",
      "47,0,[+0.05 +0.01]\n",
      "\n",
      "48,1,[+0.03 +0.03]\n",
      "49,0,[+0.06 +0.03]\n",
      "50,0,[+0.04 +0.03]\n",
      "51,1,[+0.03 +0.03]\n",
      "\n",
      "52,0,[+0.05 +0.04]\n",
      "53,1,[+0.03 +0.03]\n",
      "54,0,[+0.04 +0.03]\n",
      "55,0,[+0.05 +0.03]\n",
      "\n",
      "56,0,[+0.04 +0.03]\n",
      "57,1,[+0.03 +0.03]\n",
      "58,0,[+0.04 +0.02]\n",
      "59,1,[+0.03 +0.03]\n",
      "\n",
      "60,0,[+0.04 +0.02]\n",
      "61,0,[+0.04 +0.03]\n",
      "62,0,[+0.04 +0.03]\n",
      "63,1,[+0.03 +0.03]\n",
      "\n",
      "================================================================\n",
      "\n",
      "Difference With NN\n",
      "[[0.05642261 0.0383844 ]\n",
      " [0.02530759 0.02622006]\n",
      " [0.03455598 0.02210319]\n",
      " [0.06426933 0.04145205]\n",
      " [0.07435367 0.03366492]\n",
      " [0.0452811  0.01732897]\n",
      " [0.04979162 0.02742696]\n",
      " [0.05345149 0.03722284]\n",
      " [0.02530759 0.02622006]\n",
      " [0.05143777 0.03643558]\n",
      " [0.04129605 0.01910289]\n",
      " [0.05769862 0.03888325]\n",
      " [0.02530759 0.02622006]\n",
      " [0.05164374 0.01449667]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.0641776  0.04141619]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.07432059 0.04538156]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.0540849  0.01341   ]\n",
      " [0.07730152 0.02678615]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.00855534 0.04315142]\n",
      " [0.04318172 0.01826349]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.04400574 0.01789669]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.04855867 0.01586998]\n",
      " [0.02530759 0.02622006]\n",
      " [0.04893477 0.03545704]\n",
      " [0.02923375 0.02775498]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.02530759 0.02622006]\n",
      " [0.05248216 0.01412345]\n",
      " [0.02530759 0.02622006]\n",
      " [0.05799241 0.02607429]\n",
      " [0.04460676 0.03376502]\n",
      " [0.02530759 0.02622006]\n",
      " [0.04915112 0.03554163]\n",
      " [0.02530759 0.02622006]\n",
      " [0.04422804 0.03361696]\n",
      " [0.05333007 0.02804158]\n",
      " [0.04484087 0.03385654]\n",
      " [0.0264314  0.02665941]\n",
      " [0.04034114 0.01952796]\n",
      " [0.02530759 0.02622006]\n",
      " [0.04455911 0.01765035]\n",
      " [0.03843121 0.0313507 ]\n",
      " [0.03939989 0.03172941]\n",
      " [0.02530759 0.02622006]]\n",
      "difference [[0.003638423681201368, -0.006070729261122981], [-0.013175844028456443, -0.00872164224067827], [0.0083928852651097, -0.003967547377868093], [0.0026726168284162966, -0.002064330415017003], [0.0010885469616803423, -0.0017751342866712885], [0.007426013034753175, 0.005619156830063702], [0.012795437804054344, 0.005082464314036542], [0.0033882780390095055, -0.013956166537069884], [-0.007539509989653352, 0.016362236819357293], [0.004757934749867895, 0.0014923445208786038], [0.008510768859457125, -0.0051592394098185605], [0.00359048212676423, 0.020973292583295562], [0.010096711390240189, -0.007292489882144648], [0.007516745206989341, 0.000622807247184453], [0.006390938678699214, 0.0003016463431052864], [0.0030851029168283764, 0.0019150638782153076], [0.0014745673990876756, -0.0047258155369119745], [-0.010714499551821136, -0.016334121524609654], [0.0033195345214439044, 0.015266026573635336], [-0.00823883983717481, 0.015033612716898882], [-0.0008462340080296199, 0.008700367524428008], [0.008240729167451909, 0.03144968336550736], [0.003021696352825929, 0.013214027042412889], [-0.0059484565710720555, -0.0017620524791540255], [0.009416629458464984, 0.0002115214153532441], [0.003867221109070118, -0.004088205810494069], [-0.0060527117980425545, -0.00014264561593510874], [0.015259491089522445, 0.014338216571622306], [0.006454520584524337, 0.01307441913493814], [0.0061794037587416875, 0.007394489813939378], [-0.008387873199569941, 0.0002501393608239054], [-0.01098201327222776, 0.003901100867425518], [0.00035465610035000095, 0.010822315826178312], [0.005318674000298909, -0.0032677444916302915], [0.01845848261749982, -0.000232805902695251], [-0.0012494220314889365, 0.007406360315470359], [0.008152107601510042, 0.005815598964049644], [0.012210733074770039, 0.00787838082621651], [0.004215319828735881, 0.005836249083163363], [0.0061308743275755515, 0.003177905483595693], [0.005122793063408591, -0.0036852289395266427], [0.016431168172570912, -0.0032435845289026273], [0.010392834611777722, 0.003112900758489544], [0.00768465906885981, 0.00222121045555599], [0.012186011032633892, 0.0026149220738709567], [0.006553822174825169, 0.003054705028907735], [-0.005404218281484293, -0.007427200896643133], [0.006838636460127327, 0.00916837535575906], [-0.0033424636645880273, 0.011091323250066717], [0.004293240174042194, 0.00246691032580091], [0.0018358257451669552, -0.0045259061991385595], [0.018800477306711944, -0.0025019028121400708], [-0.0010014448086640426, 0.0018483292601034554], [0.012138304604257556, 0.00806516980250413], [0.006150919299126693, -0.0022222752380787747], [0.004309205864535391, 0.0037093013096204455], [0.00034479480437343835, -0.008016884540648729], [0.013519386619070643, 0.0008685730643881728], [0.010503214611995625, 0.007060878162311542], [0.014600045547832216, 0.013871493902673726], [0.01088457851017615, 0.006915660948453375], [0.0014586673564003227, 0.005570732040641423], [0.006252738392989385, -0.0013601175368239474], [0.010579485699032859, 0.003338876100275383]]\n",
      "Total Error: 7.311739992562468e-05\n"
     ]
    }
   ],
   "source": [
    "# Phase 1 Optimal Policy\n",
    "print(\"Phase 1 Optimal Policy\")\n",
    "print_policy(P_opt1,len(environment))\n",
    "print(\"\\nOptimal Q = \",Q_opt)\n",
    "print(\"================================================================\")\n",
    "# Phase 2 - Tabular Q-Learning Optimal Policy\n",
    "# print(\"Phase 2 - Tubular Q-Learning Optimal Policy\")\n",
    "# print(np.argmax(Q_tubular,axis=1))\n",
    "# print(\"================================================================\")\n",
    "\n",
    "# Phase 2 - DQN Optimal Policy\n",
    "print(\"Phase 2 - DQN Optimal Policy\")\n",
    "Q_NN = dql.print_dqn(optimal_network)\n",
    "print(\"================================================================\")\n",
    "\n",
    "# Output difference\n",
    "# print(\"\\nDifference With Tabular\")\n",
    "# difference,total_error = calculate_difference_and_mse(Q_opt,Q_tubular)\n",
    "# print(f\"difference {difference}\\nTotal Error: {total_error}\")\n",
    "\n",
    "# Output difference\n",
    "print(\"\\nDifference With NN\")\n",
    "print(Q_NN)\n",
    "difference,total_error = calculate_difference_and_mse(Q_opt,Q_NN)\n",
    "print(f\"difference {difference}\\nTotal Error: {total_error}\")\n",
    "different_bits = count_tables_differences(Q_opt,Q_NN)\n",
    "print(\"Different bits: \",different_bits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
