{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 2: Stock Portfolio Optimization - Assignment 3**\n",
    " Athanasakis Evangelos 2019030118  \n",
    " Fragkogiannis Yiorgos 2019030039"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **Project 2: Stock Portfolio Optimization - Assignment 3**\n",
    "# Athanasakis Evangelos 2019030118 // Fragkogiannis Yiorgos 2019030039\n",
    "\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tkinter as tk #loads standard python GUI libraries\n",
    "import random\n",
    "from tkinter import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------__________________Environments___________________-------------------------------------------------------------------------\n",
    "# Generating environments\n",
    "\n",
    "\n",
    "# Create the three different environments\n",
    "# We are modeling this environment using 8 states in the format: {stock_currently_holding,state_of_stock_1,state_of_stock_2}\n",
    "\n",
    "action_keep = 0     # keep the same stock\n",
    "action_switch = 1   # switch to the other stock\n",
    "\n",
    "# This environment is used for the question 1 where we need to demonstrate that the optimal\n",
    "# policy is always to stay with the stock we already have invested\n",
    "fee = -0.9\n",
    "# r1H = 2*r2L\n",
    "# in this case r1.h=0.1 // r2.H= 0.05 // r1.L = -0.02 // r2.L = 0.01\n",
    "# we have used a large transaction fee so that the best policy will always be to keep using the same stock\n",
    "P1 = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "            (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "            (1/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "            (9/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (1/20, 2, +0.1  + fee),  # {1,H,L}\n",
    "             (9/20, 3, +0.1  + fee)   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# This environment implements the stocks environment from the midterm\n",
    "# It is used for the question 2 where we need to demonstrate that the optimal policy\n",
    "# for some of the states is to switch and in some others to stay\n",
    "fee = -0.01\n",
    "P2 = {\n",
    "\n",
    "    # State {1,L,L}\n",
    "    0:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),    # probability: 9/20, next_State: {1,L,L}, Reward: -0.02\n",
    "             (1/20, 1, -0.02),    # {1,L,H}\n",
    "             (9/20, 2, +0.1),     # {1,H,L}\n",
    "             (1/20, 3, +0.1)      # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,L,H}\n",
    "    1:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,L}\n",
    "    2:{\n",
    "        action_keep: [\n",
    "             (9/20, 0, -0.02),  # {1,L,L}\n",
    "             (1/20, 1, -0.02),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (9/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (1/20, 5, +0.05 + fee),    # {2,L,H}\n",
    "            (9/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (1/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {1,H,H}\n",
    "    3:{\n",
    "        action_keep: [\n",
    "             (1/20, 0, -0.02),  # {1,L,L}\n",
    "             (9/20, 1, -0.02),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 ),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 )   # {1,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch: [\n",
    "            (1/20, 4, +0.01 + fee),    # {2,L,L}\n",
    "            (9/20, 5, +0.05  + fee),    # {2,L,H}\n",
    "            (1/20, 6, +0.01 + fee),    # {2,H,L}\n",
    "            (9/20, 7, +0.05 + fee)     # {2,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,L}\n",
    "    4:{\n",
    "        action_keep: [\n",
    "             (9/20, 4,  +0.01),    # {2,L,L}\n",
    "             (1/20, 5,  +0.05),    # {2,L,H}\n",
    "             (9/20, 6,  +0.01),    # {2,H,L}\n",
    "             (1/20, 7,  +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,L,H}\n",
    "    5:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "            (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "            (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "            (1/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "            (9/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,L}\n",
    "    6:{\n",
    "        action_keep: [\n",
    "             (9/20, 4, +0.01),    # {2,L,L}\n",
    "             (1/20, 5, +0.05),    # {2,L,H}\n",
    "             (9/20, 6, +0.01),    # {2,H,L}\n",
    "             (1/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (9/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (1/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (9/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (1/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # State {2,H,H}\n",
    "    7:{\n",
    "        action_keep: [\n",
    "             (1/20, 4, +0.01),    # {2,L,L}\n",
    "             (9/20, 5, +0.05),    # {2,L,H}\n",
    "             (1/20, 6, +0.01),    # {2,H,L}\n",
    "             (9/20, 7, +0.05)     # {2,H,H}\n",
    "        ],\n",
    "\n",
    "        action_switch:[\n",
    "             (1/20, 0, -0.02 + fee),  # {1,L,L}\n",
    "             (9/20, 1, -0.02 + fee),  # {1,L,H}\n",
    "             (1/20, 2, +0.1 + fee),  # {1,H,L}\n",
    "             (9/20, 3, +0.1 + fee)   # {1,H,H}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# This environment implements the generic scenario of question 3 where for every stock\n",
    "# ri_H,ri_L are chosen uniformly in [-0.02, 0.1] and transition probabilities pi_HL, pi_LH\n",
    "# are equal to 0.1 for half the stocks and 0.5 for the other half.\n",
    "\n",
    "# Since every stock can have two price states, the number of total states in the MDP\n",
    "# we are creating will be = NumOfStoscks*2^numOfStocks\n",
    "\n",
    "\n",
    "def decimal_to_binary_array(decimal, length):\n",
    "\n",
    "    # Convert decimal to binary string (strip '0b' prefix)\n",
    "    binary_string = bin(decimal)[2:]\n",
    "\n",
    "    # Determine padding length\n",
    "    padding_length = max(0, length - len(binary_string))\n",
    "\n",
    "    # Pad binary string with leading zeros if needed\n",
    "    padded_binary_string = '0' * padding_length + binary_string\n",
    "\n",
    "    # Convert padded binary string to list of binary digits\n",
    "    binary_array = [int(bit) for bit in padded_binary_string]\n",
    "\n",
    "    return binary_array\n",
    "\n",
    "\n",
    "# Function that generates the environment of N stocks dynamically, with a transaction fee\n",
    "def generate_environment(N,fee):\n",
    "\n",
    "    states_for_each_stock = 2**N\n",
    "    total_states = N * states_for_each_stock\n",
    "    max_state_length = N\n",
    "\n",
    "    P = {}\n",
    "    pi = []\n",
    "    #Creating transition probabilities for the keep action\n",
    "    #of EACH stock\n",
    "    for i in range(0,N):\n",
    "        if(i < N/2):\n",
    "            # pi_HL = pi_LH = 0.1 | # pi_HH = pi_LL = 0.9\n",
    "            row = [0.9,0.1,0.1,0.9] #[LL,LH,HL,HH]\n",
    "        else:\n",
    "            # pi_HL = pi_LH = 0.5 | # pi_HH = pi_LL = 0.5\n",
    "            row = [0.5,0.5,0.5,0.5] #[LL,LH,HL,HH]\n",
    "        pi.append(row)\n",
    "\n",
    "    progress_bar = tqdm(range(0, total_states))\n",
    "    for i in progress_bar:\n",
    "        SubDictionary={}\n",
    "        action_Keep = []\n",
    "        action_Switch = []\n",
    "\n",
    "        # find what stock we are reffering to\n",
    "        # Stock ids start from 0\n",
    "        stock = i // states_for_each_stock\n",
    "\n",
    "        ##########################\n",
    "        # We define states of L and H with binary ids\n",
    "        # For example for 2 stocks this translation occurs:\n",
    "        # LL -> 0,0 -> 0\n",
    "        # LH -> 0,1 -> 1\n",
    "        # HL -> 1,0 -> 2\n",
    "        # HH -> 1,1 -> 3\n",
    "        # The binary ids are then translated to decimals so that\n",
    "        # we can use them in code\n",
    "        ##########################\n",
    "\n",
    "        current_state = i - stock * states_for_each_stock # find where this specific stock starts at the total_states environment\n",
    "                                                          # this is necessary to calculate the transition probabilities\n",
    "\n",
    "        # Convert decimal to binary string\n",
    "        # Convert the binary string to a list of integers (0s and 1s)\n",
    "        curr_state_array = decimal_to_binary_array(current_state, max_state_length)\n",
    "        # We can now use the array to find if each stock is in high (1s) or low (0s) state\n",
    "        # So We now know that we are at state {x,L,L,H....,H} with x the number of current stock\n",
    "\n",
    "        #__Keep Stock ________________________________________________________________________________________________________________\n",
    "        # progress_1 = tqdm(range (stock*2**N, ((stock+1)*2**N)))\n",
    "        for j in range (stock*2**N, ((stock+1)*2**N)): # for every possible transition when keeping the same stock\n",
    "            state_to_trans = j - stock * states_for_each_stock          # value (H or L) of all of the stocks at the state we will transition to, in decimal form (0,1,2,3...)\n",
    "            trans_state_array = decimal_to_binary_array(state_to_trans, max_state_length) # convert to binary and take each bit separately (0 for L and 1 for H)\n",
    "\n",
    "            transitionProb = 1\n",
    "\n",
    "            for k in range(len(trans_state_array)):\n",
    "                stock_state_trans = trans_state_array[k] # 0 or 1 // low or high\n",
    "                stock_state_current = curr_state_array[k] # 0 or 1 // low or high\n",
    "\n",
    "                if(stock_state_current == 0 and stock_state_trans == 0):       # Pi_LL\n",
    "                    transitionProb = transitionProb * pi[stock][0]\n",
    "                elif(stock_state_current == 0 and stock_state_trans == 1):     # pi_LH\n",
    "                    transitionProb = transitionProb * pi[stock][1]\n",
    "                elif(stock_state_current == 1 and stock_state_trans == 0):     # pi_HL\n",
    "                    transitionProb = transitionProb * pi[stock][2]\n",
    "                else:                                                          # pi_HH\n",
    "                    transitionProb = transitionProb * pi[stock][3]\n",
    "\n",
    "            nextState = j\n",
    "            #reward = random.uniform(-0.02, 20)\n",
    "            reward = random.uniform(-0.02, 0.1)\n",
    "            action_Keep.append((transitionProb,nextState,reward))\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        #fee = 0\n",
    "        #__Switch Stock ________________________________________________________________________________________________________________\n",
    "        # progress_bar = tqdm(range (0, total_states))\n",
    "        for j in range (0, total_states): # for every possible transition when keeping the same stock\n",
    "            trans_stock = j // states_for_each_stock\n",
    "\n",
    "            if(trans_stock == stock):     # check if the transition stock is the same as the stock we start from\n",
    "                continue                  # we have already handle this situation above so we move on\n",
    "\n",
    "\n",
    "            trans_state = j - trans_stock * states_for_each_stock\n",
    "            trans_state_array = decimal_to_binary_array(trans_state, max_state_length)\n",
    "            transitionProb = 1\n",
    "\n",
    "            for k in range(len(trans_state_array)):\n",
    "                stock_state_trans = trans_state_array[k] # 0 or 1 // low or high\n",
    "                stock_state_current = curr_state_array[k] # 0 or 1 // low or high\n",
    "\n",
    "                if(stock_state_current == 0 and stock_state_trans == 0):       # Pi_LL\n",
    "                    transitionProb = transitionProb * pi[stock][0]\n",
    "                elif(stock_state_current == 0 and stock_state_trans == 1):     # pi_LH\n",
    "                    transitionProb = transitionProb * pi[stock][1]\n",
    "                elif(stock_state_current == 1 and stock_state_trans == 0):     # pi_HL\n",
    "                    transitionProb = transitionProb * pi[stock][2]\n",
    "                else:                                                          # pi_HH\n",
    "                    transitionProb = transitionProb * pi[stock][3]\n",
    "\n",
    "            nextState = j\n",
    "            #reward = random.uniform(-0.02, 20) - fee\n",
    "            reward = random.uniform(-0.02, 0.1) - fee\n",
    "            action_Switch.append((transitionProb,nextState,reward))\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        SubDictionary[action_keep] = action_Keep\n",
    "        SubDictionary[action_switch] = action_Switch\n",
    "        P[i]=SubDictionary\n",
    "\n",
    "\n",
    "\n",
    "    return P\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1, Policy Evaluation/Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(pi, P, gamma = 1.0, epsilon = 1e-10):  #inputs: (1) policy to be evaluated, (2) model of the environment (transition probabilities, etc., see previous cell), (3) discount factor (with default = 1), (4) convergence error (default = 10^{-10})\n",
    "    #print(\"in policy EVALUATION\")\n",
    "    t = 0   #there's more elegant ways to do this\n",
    "    prev_V = np.zeros(len(P)) # use as \"cost-to-go\", i.e. for V(s')\n",
    "    while True:\n",
    "        V = np.zeros(len(P)) # current value function to be learnerd\n",
    "        for s in range(len(P)):  # do for every state\n",
    "            for prob, next_state, reward in P[s][pi(s)]:  # calculate one Bellman step --> i.e., sum over all probabilities of transitions and reward for that state, the action suggested by the (fixed) policy, the reward earned (dictated by the model), and the cost-to-go from the next state (which is also decided by the model)\n",
    "                V[s] = np.int64(V[s] + prob * (reward + gamma * prev_V[next_state]))\n",
    "        if np.max(np.abs(prev_V - V)) < epsilon: #check if the new V estimate is close enough to the previous one;     \n",
    "            break # if yes, finish loop\n",
    "        prev_V = V.copy() #freeze the new values (to be used as the next V(s'))\n",
    "        t += 1\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_improvement(V, P, gamma=1.0):  # takes a value function (as the cost to go V(s')), a model, and a discount parameter\n",
    "    #print(\"in policy IMPROVEMENT\")\n",
    "    Q = np.zeros((len(P), len(P[0])), dtype=np.float64) #create a Q value array\n",
    "    for s in range(len(P)):        # for every state in the environment/model\n",
    "        for a in range(len(P[s])):  # and for every action in that state\n",
    "            for prob, next_state, reward in P[s][a]:  #evaluate the action value based on the model and Value function given (which corresponds to the previous policy that we are trying to improve) \n",
    "                Q[s][a] += prob * (reward + gamma * V[next_state])\n",
    "    new_pi = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]  # this basically creates the new (improved) policy by choosing at each state s the action a that has the highest Q value (based on the Q array we just calculated)\n",
    "    # lambda is a \"fancy\" way of creating a function without formally defining it (e.g. simply to return, as here...or to use internally in another function)\n",
    "    # you can implement this in a much simpler way, by using just a few more lines of code -- if this command is not clear, I suggest to try coding this yourself\n",
    "    \n",
    "    return new_pi,Q\n",
    "\n",
    "# policy iteration is simple, it will call alternatively policy evaluation then policy improvement, till the policy converges.\n",
    "\n",
    "def policy_iteration(P, gamma = 1.0, epsilon = 1e-10):\n",
    "    t = 0\n",
    "    random_actions = np.random.choice(tuple(P[0].keys()), len(P))     # start with random actions for each state  \n",
    "    pi = lambda s: {s:a for s, a in enumerate(random_actions)}[s]     # and define your initial policy pi_0 based on these action (remember, we are passing policies around as python \"functions\", hence the need for this second line)\n",
    "\n",
    "    while True:\n",
    "        old_pi = {s: pi(s) for s in range(len(P))}  #keep the old policy to compare with new\n",
    "        V = policy_evaluation(pi,P,gamma,epsilon)   #evaluate latest policy --> you receive its converged value function\n",
    "        pi,Q_table = policy_improvement(V,P,gamma)          #get a better policy using the value function of the previous one just calculated \n",
    "        \n",
    "        t += 1    \n",
    "        if old_pi == {s:pi(s) for s in range(len(P))}: # you have converged to the optimal policy if the \"improved\" policy is exactly the same as in the previous step\n",
    "            break\n",
    "    print('Converged after %d Policy Iterations' %t) #keep track of the number of (outer) iterations to converge\n",
    "    return V,pi,Q_table\n",
    "\n",
    "\n",
    "# Function to print policy\n",
    "def print_policy(policy, num_states=8):\n",
    "    for s in range(num_states):\n",
    "        print(f\"State {s}: Action {policy(s)}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions for Tubular Qlearning and DQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 15910.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.7290000000000001, 0, 0.07302054711684001), (0.08100000000000002, 1, 0.05357290374069412), (0.08100000000000002, 2, 0.007414369436792152), (0.009000000000000001, 3, 0.004969219222808189), (0.08100000000000002, 4, 0.030812448177156965), (0.009000000000000001, 5, 0.07355466860203551), (0.009000000000000003, 6, 0.026303406727006578), (0.0010000000000000002, 7, 0.04996314147589198)], 1: [(0.7290000000000001, 8, 0.05532403559579161), (0.08100000000000002, 9, 0.01289911176566747), (0.08100000000000002, 10, 0.0014745897802577786), (0.009000000000000001, 11, -0.024775491381420107), (0.08100000000000002, 12, -0.02340345942802441), (0.009000000000000001, 13, -0.012129112792151406), (0.009000000000000003, 14, 0.02101268944022349), (0.0010000000000000002, 15, -0.02265109587756879), (0.7290000000000001, 16, 0.08719293213534389), (0.08100000000000002, 17, 0.020756830561063908), (0.08100000000000002, 18, -0.005104747465320648), (0.009000000000000001, 19, 0.02742710197706793), (0.08100000000000002, 20, 0.01945812016702185), (0.009000000000000001, 21, 0.07548526386843586), (0.009000000000000003, 22, -0.014108260898286552), (0.0010000000000000002, 23, 0.0041301691485124605)]}, 1: {0: [(0.08100000000000002, 0, 0.06085922572673717), (0.7290000000000001, 1, 0.0116001565446168), (0.009000000000000001, 2, -0.003238060150673395), (0.08100000000000002, 3, 0.013976654822774114), (0.009000000000000001, 4, 0.02660893067554102), (0.08100000000000002, 5, 0.038376447839269745), (0.0010000000000000002, 6, 0.01746900511732739), (0.009000000000000003, 7, 0.07119175820926336)], 1: [(0.08100000000000002, 8, 0.07109928109931798), (0.7290000000000001, 9, 0.014930322681575138), (0.009000000000000001, 10, -0.020781690878251442), (0.08100000000000002, 11, 0.00998104999980001), (0.009000000000000001, 12, 0.03178075267597353), (0.08100000000000002, 13, 0.05796428450089288), (0.0010000000000000002, 14, 0.08660772092308892), (0.009000000000000003, 15, -0.025251759904930924), (0.08100000000000002, 16, 0.054890687567075026), (0.7290000000000001, 17, -0.02369981384569249), (0.009000000000000001, 18, 0.07068632406399689), (0.08100000000000002, 19, 0.03687183245811442), (0.009000000000000001, 20, 0.08625485189273963), (0.08100000000000002, 21, 0.04711106116892449), (0.0010000000000000002, 22, 0.010311227942226405), (0.009000000000000003, 23, -3.3849777162317865e-05)]}, 2: {0: [(0.08100000000000002, 0, 0.02057481909127001), (0.009000000000000001, 1, 0.034069111162539564), (0.7290000000000001, 2, 0.09494391770393704), (0.08100000000000002, 3, 0.09088434551550119), (0.009000000000000003, 4, 0.007227131731575628), (0.0010000000000000002, 5, 0.04042032552895715), (0.08100000000000002, 6, 0.04599845597510184), (0.009000000000000001, 7, -0.003457439956588936)], 1: [(0.08100000000000002, 8, 0.02048422998863495), (0.009000000000000001, 9, 0.010762059649574952), (0.7290000000000001, 10, 0.06811602770615194), (0.08100000000000002, 11, 0.0819956217268492), (0.009000000000000003, 12, 0.07293934619329068), (0.0010000000000000002, 13, 0.06062210951694428), (0.08100000000000002, 14, 0.05134350467618951), (0.009000000000000001, 15, 0.018032944750079508), (0.08100000000000002, 16, -0.0050676303178389445), (0.009000000000000001, 17, 0.08684840388832141), (0.7290000000000001, 18, 0.0021105636944118624), (0.08100000000000002, 19, 0.05974356263697344), (0.009000000000000003, 20, 0.05042211895767829), (0.0010000000000000002, 21, -0.021836178743377198), (0.08100000000000002, 22, 0.013327665204899959), (0.009000000000000001, 23, -0.0010987328901277838)]}, 3: {0: [(0.009000000000000001, 0, 0.06333403703423417), (0.08100000000000002, 1, 0.005570662766568531), (0.08100000000000002, 2, 0.026424978145728913), (0.7290000000000001, 3, 0.0968062602515708), (0.0010000000000000002, 4, 0.04826406030580241), (0.009000000000000003, 5, 0.07622869279914442), (0.009000000000000001, 6, 0.06942319585973317), (0.08100000000000002, 7, 0.024936136084248387)], 1: [(0.009000000000000001, 8, 0.03682696315500709), (0.08100000000000002, 9, -0.028505866708483485), (0.08100000000000002, 10, 0.06838353422082913), (0.7290000000000001, 11, 0.03672162482797208), (0.0010000000000000002, 12, 0.015515826088146894), (0.009000000000000003, 13, 0.08823643346513525), (0.009000000000000001, 14, -0.02357022185766087), (0.08100000000000002, 15, 0.06761703192033564), (0.009000000000000001, 16, 0.07677308413241775), (0.08100000000000002, 17, -0.017761188439766293), (0.08100000000000002, 18, 0.02439916888381568), (0.7290000000000001, 19, -0.025368634510980082), (0.0010000000000000002, 20, 0.057417136236651754), (0.009000000000000003, 21, -0.006434286645118957), (0.009000000000000001, 22, -0.014367396418575628), (0.08100000000000002, 23, -0.026716398390704228)]}, 4: {0: [(0.08100000000000002, 0, 0.019542159264696286), (0.009000000000000001, 1, 0.08703871503897707), (0.009000000000000003, 2, 0.018210514413455493), (0.0010000000000000002, 3, 0.02792301996103509), (0.7290000000000001, 4, 0.09625107595379569), (0.08100000000000002, 5, 0.007605363017248547), (0.08100000000000002, 6, 0.0836412230637094), (0.009000000000000001, 7, -0.005056405693194524)], 1: [(0.08100000000000002, 8, 0.07956119443621465), (0.009000000000000001, 9, -0.006191873514543726), (0.009000000000000003, 10, 0.02240861918379599), (0.0010000000000000002, 11, 0.04210417230036178), (0.7290000000000001, 12, 0.028090435921601063), (0.08100000000000002, 13, 0.07812796370109223), (0.08100000000000002, 14, 0.03746790474451773), (0.009000000000000001, 15, -0.00861337761581077), (0.08100000000000002, 16, 0.03810265239927558), (0.009000000000000001, 17, 0.009607675588830645), (0.009000000000000003, 18, -0.006588960560282868), (0.0010000000000000002, 19, 0.04857915910357392), (0.7290000000000001, 20, -0.02296883042214878), (0.08100000000000002, 21, 0.052168106661710005), (0.08100000000000002, 22, 0.028877378545840722), (0.009000000000000001, 23, -0.022109222568969997)]}, 5: {0: [(0.009000000000000001, 0, 0.0959871807259604), (0.08100000000000002, 1, -0.014826455671768182), (0.0010000000000000002, 2, 0.02945564874365458), (0.009000000000000003, 3, 0.010280413258412075), (0.08100000000000002, 4, -0.0032866740685475668), (0.7290000000000001, 5, 0.0916389656770286), (0.009000000000000001, 6, 0.08631778539131162), (0.08100000000000002, 7, 0.021220326886449304)], 1: [(0.009000000000000001, 8, 0.05627921210776463), (0.08100000000000002, 9, 0.06912393344157501), (0.0010000000000000002, 10, -0.026855475285872225), (0.009000000000000003, 11, 0.0855211470858594), (0.08100000000000002, 12, -0.017159459167696473), (0.7290000000000001, 13, -0.01133688635928549), (0.009000000000000001, 14, -0.0031328822176841468), (0.08100000000000002, 15, 0.08350880427611779), (0.009000000000000001, 16, 0.08847910716688182), (0.08100000000000002, 17, 0.08827769040062117), (0.0010000000000000002, 18, 0.024991120980135083), (0.009000000000000003, 19, 0.055452448579983195), (0.08100000000000002, 20, -0.00309803656560448), (0.7290000000000001, 21, 0.08476614791972943), (0.009000000000000001, 22, -0.028765966419853538), (0.08100000000000002, 23, 0.0070035005373268785)]}, 6: {0: [(0.009000000000000003, 0, -0.0192005392545778), (0.0010000000000000002, 1, 0.03836883336611556), (0.08100000000000002, 2, 0.06738970620451465), (0.009000000000000001, 3, 0.019774087554847993), (0.08100000000000002, 4, 0.09102598783117358), (0.009000000000000001, 5, -0.015023525527343304), (0.7290000000000001, 6, 0.040123440427038776), (0.08100000000000002, 7, 0.008728546515742015)], 1: [(0.009000000000000003, 8, 0.06318091116377018), (0.0010000000000000002, 9, 0.08431216651442122), (0.08100000000000002, 10, 0.054445366945851316), (0.009000000000000001, 11, 0.021107609508792737), (0.08100000000000002, 12, -0.02691868130620037), (0.009000000000000001, 13, 0.05740327308936823), (0.7290000000000001, 14, 0.05614842883924367), (0.08100000000000002, 15, 0.02710277948513034), (0.009000000000000003, 16, 0.0462278047843312), (0.0010000000000000002, 17, 0.08713611242661849), (0.08100000000000002, 18, 0.047391337246093075), (0.009000000000000001, 19, 0.047542242819296), (0.08100000000000002, 20, 0.05343229411882088), (0.009000000000000001, 21, 0.04433963881182868), (0.7290000000000001, 22, 0.030756443061285492), (0.08100000000000002, 23, -0.01096073715709337)]}, 7: {0: [(0.0010000000000000002, 0, 0.06594059936902637), (0.009000000000000003, 1, 0.02569566747629001), (0.009000000000000001, 2, 0.07359562084447081), (0.08100000000000002, 3, 0.0893247664977879), (0.009000000000000001, 4, 0.07828663854872633), (0.08100000000000002, 5, 0.07607455338406167), (0.08100000000000002, 6, -0.007858926001891904), (0.7290000000000001, 7, 0.02860451420919539)], 1: [(0.0010000000000000002, 8, 0.06152227448141879), (0.009000000000000003, 9, -0.018213103330359214), (0.009000000000000001, 10, 0.03928548783881527), (0.08100000000000002, 11, -0.026095269264666654), (0.009000000000000001, 12, 0.034200016522503394), (0.08100000000000002, 13, 0.08580885215029507), (0.08100000000000002, 14, -0.02225345043746764), (0.7290000000000001, 15, 0.08695731916892624), (0.0010000000000000002, 16, 0.022703918753629503), (0.009000000000000003, 17, 0.030743390146791248), (0.009000000000000001, 18, 0.0665388396334832), (0.08100000000000002, 19, 0.0661370706120613), (0.009000000000000001, 20, 0.06433400805399804), (0.08100000000000002, 21, -0.01486035428712705), (0.08100000000000002, 22, -0.027665049273022872), (0.7290000000000001, 23, 0.0861151379602483)]}, 8: {0: [(0.7290000000000001, 8, 0.026451149266159418), (0.08100000000000002, 9, 0.038040426526380594), (0.08100000000000002, 10, 0.08954167042842598), (0.009000000000000001, 11, 0.022417848199766794), (0.08100000000000002, 12, 0.014857578890146477), (0.009000000000000001, 13, 0.0005775153089033827), (0.009000000000000003, 14, 0.08489550147339603), (0.0010000000000000002, 15, 0.06442732941468139)], 1: [(0.7290000000000001, 0, 0.05196784044171165), (0.08100000000000002, 1, 0.07120151756014993), (0.08100000000000002, 2, 0.02297412742421335), (0.009000000000000001, 3, 0.055189085953434115), (0.08100000000000002, 4, 0.06266766690493104), (0.009000000000000001, 5, 0.08013396653827716), (0.009000000000000003, 6, 0.08635087082446494), (0.0010000000000000002, 7, 0.049545144076344035), (0.7290000000000001, 16, -0.016667253474705022), (0.08100000000000002, 17, -0.0002476571232432386), (0.08100000000000002, 18, 0.04069708858920173), (0.009000000000000001, 19, 0.07695776472604317), (0.08100000000000002, 20, 0.016547850582363), (0.009000000000000001, 21, 0.0055995600394886155), (0.009000000000000003, 22, 0.05537796695162698), (0.0010000000000000002, 23, 0.06705028759059414)]}, 9: {0: [(0.08100000000000002, 8, 0.06708772356844464), (0.7290000000000001, 9, 0.020014676427302123), (0.009000000000000001, 10, -0.006537204334633491), (0.08100000000000002, 11, -0.009584175930980904), (0.009000000000000001, 12, 0.01876264945292284), (0.08100000000000002, 13, 0.09456883876515552), (0.0010000000000000002, 14, 0.0018346512626109673), (0.009000000000000003, 15, 0.0987269957093772)], 1: [(0.08100000000000002, 0, 0.029984637498393703), (0.7290000000000001, 1, 0.057844879577254794), (0.009000000000000001, 2, 0.03291414686221684), (0.08100000000000002, 3, 0.036094538933095076), (0.009000000000000001, 4, 0.02005017172753852), (0.08100000000000002, 5, 0.013468201646562246), (0.0010000000000000002, 6, 0.047879825374261394), (0.009000000000000003, 7, -0.0016989514378025292), (0.08100000000000002, 16, 0.03224518000817692), (0.7290000000000001, 17, 0.052784271248063085), (0.009000000000000001, 18, 0.017877802309442484), (0.08100000000000002, 19, 0.07114092189785622), (0.009000000000000001, 20, 0.013848880498603144), (0.08100000000000002, 21, -0.021437959013241942), (0.0010000000000000002, 22, 0.01331340162515286), (0.009000000000000003, 23, 0.08374020172558792)]}, 10: {0: [(0.08100000000000002, 8, 0.062085178704249316), (0.009000000000000001, 9, 0.08418385606310666), (0.7290000000000001, 10, 0.027651199439327426), (0.08100000000000002, 11, 0.01995307232274941), (0.009000000000000003, 12, 0.06997982425170476), (0.0010000000000000002, 13, 0.0838053702431118), (0.08100000000000002, 14, 0.058704276138783484), (0.009000000000000001, 15, 0.09922870518377115)], 1: [(0.08100000000000002, 0, -0.006508653241045737), (0.009000000000000001, 1, 0.053239955412886834), (0.7290000000000001, 2, 0.07535962111483994), (0.08100000000000002, 3, -0.026063058554389486), (0.009000000000000003, 4, -0.020750866416122254), (0.0010000000000000002, 5, 0.031968897215915164), (0.08100000000000002, 6, -0.007894480340048757), (0.009000000000000001, 7, 0.05953891331799018), (0.08100000000000002, 16, 0.022966545939608805), (0.009000000000000001, 17, 0.07286887191822249), (0.7290000000000001, 18, 0.05556848825230672), (0.08100000000000002, 19, 0.029035985472795768), (0.009000000000000003, 20, 0.06213938328951931), (0.0010000000000000002, 21, 0.050760226959577305), (0.08100000000000002, 22, 0.04916532232536313), (0.009000000000000001, 23, 0.011515183407408957)]}, 11: {0: [(0.009000000000000001, 8, 0.09344111877097004), (0.08100000000000002, 9, 0.06037634771486354), (0.08100000000000002, 10, 0.031728240865264865), (0.7290000000000001, 11, 0.0945265124687204), (0.0010000000000000002, 12, 0.03599505683500055), (0.009000000000000003, 13, 0.07396182361533359), (0.009000000000000001, 14, 0.005567567607795181), (0.08100000000000002, 15, 0.0037312346805116955)], 1: [(0.009000000000000001, 0, 0.04918106762065278), (0.08100000000000002, 1, 0.0722794382744655), (0.08100000000000002, 2, 0.0041763882400508), (0.7290000000000001, 3, -0.009886464766957854), (0.0010000000000000002, 4, 0.05933485432922763), (0.009000000000000003, 5, 0.013756801808323746), (0.009000000000000001, 6, 0.08409176310632073), (0.08100000000000002, 7, -0.021718338036363416), (0.009000000000000001, 16, 0.03882308299107277), (0.08100000000000002, 17, 0.02616235443172344), (0.08100000000000002, 18, 0.05797633331623136), (0.7290000000000001, 19, 0.06763837041280234), (0.0010000000000000002, 20, 0.05073787630899367), (0.009000000000000003, 21, -0.0014519541549653115), (0.009000000000000001, 22, -0.024220953723280467), (0.08100000000000002, 23, 0.012385868755890405)]}, 12: {0: [(0.08100000000000002, 8, 0.03563470246667984), (0.009000000000000001, 9, 0.0365009280399808), (0.009000000000000003, 10, 0.06437401713800102), (0.0010000000000000002, 11, 0.058957083279298245), (0.7290000000000001, 12, 0.06003746813739591), (0.08100000000000002, 13, 0.04585203198373544), (0.08100000000000002, 14, 0.046297695518448784), (0.009000000000000001, 15, 0.07401049059396844)], 1: [(0.08100000000000002, 0, 0.08818077706735934), (0.009000000000000001, 1, 0.012667901349622595), (0.009000000000000003, 2, -0.023657119943461615), (0.0010000000000000002, 3, 0.06916051949144023), (0.7290000000000001, 4, 0.058425808239579), (0.08100000000000002, 5, 0.07924256756867096), (0.08100000000000002, 6, -0.0047199145083200635), (0.009000000000000001, 7, -0.024099649221714724), (0.08100000000000002, 16, 0.03145758540195943), (0.009000000000000001, 17, 0.0805543480472155), (0.009000000000000003, 18, 0.06717087853434217), (0.0010000000000000002, 19, 0.005983619576226581), (0.7290000000000001, 20, 0.016658410908861276), (0.08100000000000002, 21, -0.014647077517640352), (0.08100000000000002, 22, 0.0814647070543664), (0.009000000000000001, 23, 0.03798561701896338)]}, 13: {0: [(0.009000000000000001, 8, 0.09070809946025282), (0.08100000000000002, 9, -0.019785428860246132), (0.0010000000000000002, 10, 0.07675320307742972), (0.009000000000000003, 11, 0.0696965891997315), (0.08100000000000002, 12, 0.011746167122495415), (0.7290000000000001, 13, 0.003920505893856757), (0.009000000000000001, 14, -0.0008408065538136333), (0.08100000000000002, 15, 0.02228077345140846)], 1: [(0.009000000000000001, 0, 0.0025200826485745464), (0.08100000000000002, 1, -0.028587021339972395), (0.0010000000000000002, 2, 0.07172466701096221), (0.009000000000000003, 3, 0.06524754546175586), (0.08100000000000002, 4, 0.05995743734487977), (0.7290000000000001, 5, 0.018868973217254816), (0.009000000000000001, 6, 0.06831490807338285), (0.08100000000000002, 7, -0.018429944412486816), (0.009000000000000001, 16, 0.06596843425943415), (0.08100000000000002, 17, -0.011675737481645057), (0.0010000000000000002, 18, 0.06747668440793916), (0.009000000000000003, 19, 0.051999769498884955), (0.08100000000000002, 20, 0.07485744974044586), (0.7290000000000001, 21, -0.010134813127752119), (0.009000000000000001, 22, -0.017995119154598387), (0.08100000000000002, 23, -0.026950738267727807)]}, 14: {0: [(0.009000000000000003, 8, 0.09232631986247544), (0.0010000000000000002, 9, 0.09875122992896083), (0.08100000000000002, 10, 0.01836818743749354), (0.009000000000000001, 11, 0.029532102500873656), (0.08100000000000002, 12, 0.002556832778179332), (0.009000000000000001, 13, 1.1449424970107897e-05), (0.7290000000000001, 14, 0.09481470365946572), (0.08100000000000002, 15, 0.07136448692577528)], 1: [(0.009000000000000003, 0, -0.017775713873515002), (0.0010000000000000002, 1, -0.0022363200814110463), (0.08100000000000002, 2, -0.0060646587369445875), (0.009000000000000001, 3, -0.012121726379618723), (0.08100000000000002, 4, 0.0632309899884918), (0.009000000000000001, 5, 0.02784990798594659), (0.7290000000000001, 6, 0.06587042030050948), (0.08100000000000002, 7, 0.07351931655675605), (0.009000000000000003, 16, 0.002443793690827532), (0.0010000000000000002, 17, 0.06942635877619649), (0.08100000000000002, 18, 0.05781269943096428), (0.009000000000000001, 19, 0.058887808323611464), (0.08100000000000002, 20, 0.05155296248969308), (0.009000000000000001, 21, 0.06119390150502759), (0.7290000000000001, 22, 0.05410376945327682), (0.08100000000000002, 23, 0.024450886800867162)]}, 15: {0: [(0.0010000000000000002, 8, 0.0005030541320694033), (0.009000000000000003, 9, -0.012582554755376028), (0.009000000000000001, 10, 0.03886086248878837), (0.08100000000000002, 11, 0.020764675534621153), (0.009000000000000001, 12, 0.008997331557858899), (0.08100000000000002, 13, 0.04938769683122006), (0.08100000000000002, 14, 0.09265798909069604), (0.7290000000000001, 15, 0.007455419013987264)], 1: [(0.0010000000000000002, 0, 0.008835869797789614), (0.009000000000000003, 1, 0.0006870921346012557), (0.009000000000000001, 2, -0.026324717536928374), (0.08100000000000002, 3, 0.07892942947680485), (0.009000000000000001, 4, 0.08040557162527141), (0.08100000000000002, 5, -0.002717902594002416), (0.08100000000000002, 6, 0.014620000782075105), (0.7290000000000001, 7, -0.014159038306577608), (0.0010000000000000002, 16, 0.04313105889523262), (0.009000000000000003, 17, -0.013553250454976072), (0.009000000000000001, 18, -0.028414077885878644), (0.08100000000000002, 19, 0.002756527444439492), (0.009000000000000001, 20, 0.0739774046739625), (0.08100000000000002, 21, 0.0836623708656824), (0.08100000000000002, 22, 0.005954691887460379), (0.7290000000000001, 23, 0.07070541500266217)]}, 16: {0: [(0.125, 16, -0.0049092313863060375), (0.125, 17, -0.0011452481316441034), (0.125, 18, -0.01503846346787436), (0.125, 19, -6.884254733143316e-05), (0.125, 20, -0.0067804914730747335), (0.125, 21, 0.09097702691714023), (0.125, 22, 0.09946170299412672), (0.125, 23, 0.08886045248148802)], 1: [(0.125, 0, -0.010939381015147699), (0.125, 1, 0.03494057868280339), (0.125, 2, 0.0645549680946043), (0.125, 3, 0.0011199266332195344), (0.125, 4, 0.05534593608324478), (0.125, 5, 0.035976576581626805), (0.125, 6, 0.0340654434223349), (0.125, 7, 0.029923314939670327), (0.125, 8, 0.016790770603973916), (0.125, 9, 0.011795678423626518), (0.125, 10, 0.03419546735822967), (0.125, 11, 0.025598741690982633), (0.125, 12, 0.07591268981410464), (0.125, 13, 0.016821384683482626), (0.125, 14, -0.01705632628778206), (0.125, 15, 0.06834135168288026)]}, 17: {0: [(0.125, 16, 0.07969954213920509), (0.125, 17, 0.08064573630698665), (0.125, 18, 0.0846915591883961), (0.125, 19, -0.018640169686243952), (0.125, 20, 0.05104183649666513), (0.125, 21, 0.04587960171909744), (0.125, 22, -0.019740454678531014), (0.125, 23, 0.041436175515764714)], 1: [(0.125, 0, 0.06904267620767486), (0.125, 1, -0.0014847780555366733), (0.125, 2, 0.004898168907849476), (0.125, 3, -0.008281515873205606), (0.125, 4, 0.07296821948250594), (0.125, 5, 0.02633192694688296), (0.125, 6, 0.04931925007815418), (0.125, 7, 0.010158720105822866), (0.125, 8, 0.04086354578375866), (0.125, 9, 0.049198239792916106), (0.125, 10, 0.00726439365855854), (0.125, 11, 0.009331503790655856), (0.125, 12, -0.003693113039340992), (0.125, 13, -0.02576371591571023), (0.125, 14, 0.044791952084968674), (0.125, 15, 0.011679984214097888)]}, 18: {0: [(0.125, 16, 0.013468028409741418), (0.125, 17, 0.019538861288964034), (0.125, 18, 0.056008280797691684), (0.125, 19, 0.06991547234047266), (0.125, 20, 0.04395947605823715), (0.125, 21, 0.045976045538981714), (0.125, 22, -0.013539149310614004), (0.125, 23, 0.031209432688617995)], 1: [(0.125, 0, 0.08158018168697254), (0.125, 1, -0.01872573235834129), (0.125, 2, 0.07441698359709922), (0.125, 3, 0.05055114286791653), (0.125, 4, 0.03675937092460805), (0.125, 5, -0.013026275431265838), (0.125, 6, 0.08367343286299443), (0.125, 7, 0.038567456837114246), (0.125, 8, -0.00945377641732032), (0.125, 9, 0.08263455664445736), (0.125, 10, 0.06287995247891182), (0.125, 11, 0.03879896995738772), (0.125, 12, 0.04882151114095493), (0.125, 13, 0.05806881513513953), (0.125, 14, 0.077794888360044), (0.125, 15, 0.04703839354782354)]}, 19: {0: [(0.125, 16, 0.03701467666825674), (0.125, 17, -0.011916404866455305), (0.125, 18, 0.023118916337652274), (0.125, 19, 0.08354138556180316), (0.125, 20, 0.09838771692694813), (0.125, 21, 0.026786327065225147), (0.125, 22, 0.05323620137122001), (0.125, 23, 0.03202156544250645)], 1: [(0.125, 0, -0.013612649478958393), (0.125, 1, -0.013799353692254117), (0.125, 2, -0.024657433250203815), (0.125, 3, 0.03703215306702364), (0.125, 4, 0.015431245638048067), (0.125, 5, 0.03967961644283046), (0.125, 6, 0.07866952520738509), (0.125, 7, 0.004669252212830903), (0.125, 8, 0.03523694173390026), (0.125, 9, 0.04887554693114949), (0.125, 10, 0.013081196071723996), (0.125, 11, 0.007072938003960983), (0.125, 12, -0.0215697364411494), (0.125, 13, -0.019933520879959907), (0.125, 14, 0.04489115269484751), (0.125, 15, -0.021841665519258757)]}, 20: {0: [(0.125, 16, 0.03739899986434586), (0.125, 17, 0.06586570094875885), (0.125, 18, 0.05924762995135789), (0.125, 19, 0.04575525292586159), (0.125, 20, 0.07735704000724095), (0.125, 21, 0.061495338236300406), (0.125, 22, 0.025641015761137694), (0.125, 23, -0.009428721223235178)], 1: [(0.125, 0, 0.023751685162944784), (0.125, 1, 0.07529990292483364), (0.125, 2, -0.02368117213389443), (0.125, 3, -0.011199947017934095), (0.125, 4, 0.04327640695538474), (0.125, 5, 0.03937788596152802), (0.125, 6, 0.05658198400669757), (0.125, 7, 0.048762837528682705), (0.125, 8, 0.06294412236821906), (0.125, 9, 0.02324786071399796), (0.125, 10, 0.07065896244856913), (0.125, 11, 0.026480841460235015), (0.125, 12, 0.02822769420045359), (0.125, 13, -0.014755894034956767), (0.125, 14, 0.053752580284971034), (0.125, 15, 0.07564102000178871)]}, 21: {0: [(0.125, 16, 0.0022338059114802404), (0.125, 17, 0.03613142913767094), (0.125, 18, -0.0036360917229703825), (0.125, 19, 0.018608078513618367), (0.125, 20, 0.07874701665937989), (0.125, 21, -0.017768174197743982), (0.125, 22, 0.08776949212128021), (0.125, 23, 0.03455826666679075)], 1: [(0.125, 0, 0.08650651550672862), (0.125, 1, -0.008587829062224583), (0.125, 2, 0.024580224357183718), (0.125, 3, -0.014556363032337186), (0.125, 4, 0.046073086750333746), (0.125, 5, 0.06354709428049564), (0.125, 6, -0.009296549402673595), (0.125, 7, 0.02947156134529596), (0.125, 8, 0.05884204226637666), (0.125, 9, -0.017329986599879103), (0.125, 10, -0.02038169562154598), (0.125, 11, 0.03988450877148663), (0.125, 12, 0.015714251209689445), (0.125, 13, -0.004042202504473942), (0.125, 14, -0.007212409107517413), (0.125, 15, 0.0736526485877815)]}, 22: {0: [(0.125, 16, 0.04664945107265461), (0.125, 17, 0.07568166072454226), (0.125, 18, 0.030223606651899482), (0.125, 19, 0.03473698282232823), (0.125, 20, 0.024302749790837432), (0.125, 21, 0.024995529407414636), (0.125, 22, 0.0808238278424859), (0.125, 23, 0.044589015300152954)], 1: [(0.125, 0, 0.006379190446619115), (0.125, 1, 0.04148079623638636), (0.125, 2, -0.016933250958150625), (0.125, 3, 0.01630877884480572), (0.125, 4, 0.02090490379960512), (0.125, 5, 0.06845984512055424), (0.125, 6, 0.028122205360761036), (0.125, 7, 0.023893672949963897), (0.125, 8, -0.007497244129176265), (0.125, 9, 0.06498431456214968), (0.125, 10, 0.0657943466084925), (0.125, 11, 0.017824380347495936), (0.125, 12, 0.023373997310042187), (0.125, 13, 0.06222152377436938), (0.125, 14, -0.01645194200621076), (0.125, 15, 0.028935867497487826)]}, 23: {0: [(0.125, 16, 0.08011110978003318), (0.125, 17, -0.002806661347140811), (0.125, 18, 0.02350000578016372), (0.125, 19, 0.03557755777547228), (0.125, 20, 0.0672499186855638), (0.125, 21, 0.08618942016367254), (0.125, 22, 0.07304136484530355), (0.125, 23, 0.07309713409086531)], 1: [(0.125, 0, 0.02521258288415589), (0.125, 1, 0.07695467775177142), (0.125, 2, 0.0528497165376765), (0.125, 3, 0.06820195549982493), (0.125, 4, -0.0068811633549657625), (0.125, 5, 0.036148431255104564), (0.125, 6, 0.08625299740972853), (0.125, 7, 0.01316029554815478), (0.125, 8, 0.04973379571054689), (0.125, 9, 0.004453385126544215), (0.125, 10, -0.020604957156687385), (0.125, 11, -0.019442682174797665), (0.125, 12, 0.062254982432006147), (0.125, 13, 0.04176833896586827), (0.125, 14, 0.055238913339793715), (0.125, 15, 0.014072800898882678)]}}\n",
      "\n",
      "Response:  [(0.08100000000000002, 8, 0.07109928109931798), (0.7290000000000001, 9, 0.014930322681575138), (0.009000000000000001, 10, -0.020781690878251442), (0.08100000000000002, 11, 0.00998104999980001), (0.009000000000000001, 12, 0.03178075267597353), (0.08100000000000002, 13, 0.05796428450089288), (0.0010000000000000002, 14, 0.08660772092308892), (0.009000000000000003, 15, -0.025251759904930924), (0.08100000000000002, 16, 0.054890687567075026), (0.7290000000000001, 17, -0.02369981384569249), (0.009000000000000001, 18, 0.07068632406399689), (0.08100000000000002, 19, 0.03687183245811442), (0.009000000000000001, 20, 0.08625485189273963), (0.08100000000000002, 21, 0.04711106116892449), (0.0010000000000000002, 22, 0.010311227942226405), (0.009000000000000003, 23, -3.3849777162317865e-05)]\n",
      "\n",
      "Choices:  range(0, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 0.014930322681575138)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_difference_and_mse(q1, q2):\n",
    "    # Ensure the tables have the same dimensions\n",
    "    if len(q1) != len(q2) or any(len(row1) != len(row2) for row1, row2 in zip(q1, q2)):\n",
    "        raise ValueError(\"Both tables must have the same dimensions.\")\n",
    "    \n",
    "    result = []\n",
    "    total_squared_error = 0\n",
    "    num_elements = 0\n",
    "    \n",
    "    for row1, row2 in zip(q1, q2):\n",
    "        row_diff = []\n",
    "        for element1, element2 in zip(row1, row2):\n",
    "            diff = element1 - element2\n",
    "            row_diff.append(diff)\n",
    "            total_squared_error += diff ** 2\n",
    "            num_elements += 1\n",
    "        result.append(row_diff)\n",
    "    \n",
    "    mse = total_squared_error / num_elements\n",
    "    return result, mse\n",
    "\n",
    "\n",
    "def check_q_table_convergence(prev_Q, current_Q, epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Checks if the Q-table has converged.\n",
    "\n",
    "    Parameters:\n",
    "    - prev_Q (np.ndarray): Previous Q-table.\n",
    "    - current_Q (np.ndarray): Current Q-table.\n",
    "    - epsilon (float): Convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if Q-table has converged, False otherwise.\n",
    "    \"\"\"\n",
    "    if prev_Q is None:\n",
    "        return False  # Cannot determine convergence without a previous Q-table\n",
    "    \n",
    "    # Calculate the maximum absolute difference between corresponding Q-values\n",
    "    max_diff = np.max(np.abs(prev_Q - current_Q))\n",
    "    \n",
    "    # Check if the maximum difference is less than epsilon\n",
    "    if max_diff < epsilon:\n",
    "        return True  # Q-table has converged\n",
    "    \n",
    "    return False  # Q-table has not converged yet\n",
    "\n",
    "\n",
    "# This function is used to simulate the environments response\n",
    "# It gets as input the environment, the current state and the action that we have selected\n",
    "# and it returns the next state and the reward\n",
    "def get_response(environment, state, action):\n",
    "    P = environment\n",
    "    print(P)\n",
    "    response = P[state][action] # get next states, transition probabilities and transaction rewards\n",
    "                                # based on the current state and the action we want to make   \n",
    "\n",
    "    #print(\"\\nResponse: \",response)\n",
    "\n",
    "    # we use random.choices to get a random next state based on the weighted probabilities of the next states\n",
    "    probabilities = []\n",
    "    choices = range(len(P[state][action]))\n",
    "    for i in range(len(P[state][action])): \n",
    "        probabilities.append(response[i][0])\n",
    "     \n",
    "    #print(\"\\nChoices: \",choices)\n",
    "\n",
    "    # because depending on the action (keep or switch) the num of actions we can take is different\n",
    "    # hence, we check what the action we do is and declare the choices array accordingly\n",
    "        \n",
    "    # Make a random choice based on probabilities\n",
    "    # k=1: Specifies that we want to make a single random choice.\n",
    "    # [0] is used to extract the single element from that list\n",
    "    random_choice = random.choices(choices, weights=probabilities, k=1)[0]\n",
    "     \n",
    "    next_state = response [random_choice][1] # get next state\n",
    "    reward = response [random_choice][2]     # get reward\n",
    "     \n",
    "    return next_state,reward\n",
    "\n",
    "# test_p = generate_environment(3,0.01)\n",
    "# #for i in range(10):\n",
    "# get_response(test_p,1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2\n",
    " Implementing Tubular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================================================\n",
    "#################### Q-Learning ################\n",
    "#===== Hyperparameters ===================\n",
    "# alpha -> Learning rate\n",
    "# gamma -> Discount factor\n",
    "# epsilon ->  # Exploration rate\n",
    "# epsilon_decay -> Decay rate for epsilon\n",
    "# min_epsilon -> Minimum epsilon value\n",
    "# num_episodes -> Number of episodes\n",
    "\n",
    "def implement_Q_learning(env, num_of_episodes, alpha, gamma, epsilon_decay=0.999, alpha_decay=0.001, finding_parameters=False):\n",
    "    Q = np.zeros((len(env),len(env[0])))\n",
    "    epsilon = 1.0                # Exploration rate0\n",
    "    #epsilon_decay = 0.99        # Decay rate for epsilon\n",
    "    min_epsilon = 0.1            # Minimum epsilon value\n",
    "    #alpha_decay = 0.01\n",
    "    initial_alpha = alpha\n",
    "    min_alpha = 0.001\n",
    "    convergence_episode = float('inf')  # Initialize with a large number\n",
    "    conv_counter = 0\n",
    "\n",
    "    progress_bar = tqdm(range(num_of_episodes))\n",
    "\n",
    "    for episode in progress_bar: \n",
    "        prev_Q = np.copy(Q)\n",
    "        #current_state = random.choices(len(environment)-1, k=1)[0] \n",
    "        current_state = random.randint(0, len(env)-1) # select a random starting state\n",
    "        #print(\"Currene State: \",current_state)\n",
    "        for _ in range(100):      # do 100 steps do get a feel for what happens in the environment\n",
    "            # decide if we are going to explore or to exploit based on the epsilon value\n",
    "            # if random.uniform(0,1) < epsilon:\n",
    "            #     # Explore by picking a random action\n",
    "            #     action = random.choice([0,1])\n",
    "            #     #print(\"EXLORE ACTION: \",action)\n",
    "            # else:\n",
    "            #     action = np.argmax(Q[current_state])\n",
    "            #print(\"action\",action)\n",
    "            actions = [0,1]\n",
    "            action = random.choices([random.choices(actions, k=1)[0], np.argmax(Q[current_state])], weights=[epsilon, 1 - epsilon], k=1)[0]\n",
    "\n",
    "            next_state,reward = get_response(env, current_state, action)\n",
    "            #print(f\"Next State {next_state}\")\n",
    "            \n",
    "            Q[current_state,action] = Q[current_state,action] + alpha * (\n",
    "                reward + gamma * np.max(Q[next_state]) - Q[current_state,action]\n",
    "            )\n",
    "            #Q[current_state, action - 1] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[current_state, action - 1])\n",
    "            #print(f\"Q[{current_state},{action}] = {Q[current_state][action]}\")\n",
    "            # update the current state\n",
    "            current_state = next_state    \n",
    "        \n",
    "        # update the hyperparameters     \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        alpha = max(min_alpha, initial_alpha * np.exp(-alpha_decay * episode))\n",
    "\n",
    "        #print(\"epsilon: \",epsilon)\n",
    "        if finding_parameters == True and check_q_table_convergence(prev_Q, Q, epsilon=0.00002):\n",
    "            conv_counter += 1\n",
    "            if conv_counter > 2:  # Adjust convergence criteria based on your problem\n",
    "                # convergence_episode = min(convergence_episode, episode)\n",
    "                convergence_episode = episode\n",
    "                # print(\"prev_Q:\", prev_Q)\n",
    "                # print(\"Q:\", Q)\n",
    "                print(\"convergence_episode = \",convergence_episode)\n",
    "                # print(np.argmax(Q,axis=1))\n",
    "                conv_counter = 0\n",
    "                break\n",
    "\n",
    "    # print(\"\\n\",Q)\n",
    "    return Q, convergence_episode\n",
    "\n",
    "\n",
    "\n",
    "# environment = P2\n",
    "# alpha = 0.5\n",
    "# gamma = 0\n",
    "# V_opt1,P_opt1,Q_opt = policy_iteration(environment,gamma)\n",
    "\n",
    "\n",
    "# Define objective function for Optuna\n",
    "# Optuna tries to minimise the output of the objective function by modifying the hyperparameters of the tubular q learning algorithm\n",
    "# The output of the function will be the mse of the policy found at convergence summed up with the number of steps it took to converge\n",
    "# Because finding a correct policy is more important then the number of steps, mse is (weighted) multiplied with 10^14 (so that it has greater impact on \n",
    "# the output of the objective function)\n",
    "#def objective(trial):    \n",
    "    #environment = P2  # Define your environment here\n",
    "    num_of_episodes = 100000  # Adjust as needed\n",
    "    alpha = trial.suggest_float('alpha', 0.5, 0.9, log=True)\n",
    "    gamma = 0\n",
    "    epsilon_decay = trial.suggest_float('epsilon_decay', 0.95, 0.999)\n",
    "    alpha_decay = trial.suggest_float('alpha_decay', 0.001, 0.01)\n",
    "    finding_parameters = True    \n",
    "    Q, convergence_episode = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\n",
    "    print(np.argmax(Q,axis=1))\n",
    "    \n",
    "    # Return the inverse of convergence episode (maximize speed)\n",
    "    convergence_episode = convergence_episode if convergence_episode != float('inf') else 10000\n",
    "    r, mse = calculate_difference_and_mse(Q_opt, Q)   \n",
    "    difference_count = sum(1 for x, y in zip(np.argmax(Q_opt,axis=1), np.argmax(Q,axis=1)) if x != y)\n",
    "    result = mse * 10000000000000000 * (difference_count+1) + convergence_episode/10 \n",
    "    print(\"mse: \",mse,\" result: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# def count_tables_differences(table1, table2):\n",
    "#     if len(table1) != len(table2):\n",
    "#         raise ValueError(\"Both tables must have the same length.\")\n",
    "    \n",
    "#     difference_count = sum(1 for x, y in zip(table1, table2) if x != y)\n",
    "#     return difference_count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Deep Q-Learning Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################____TASK3____########################################\n",
    "\n",
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Define model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
    "        self.out = nn.Linear(h1_nodes, out_actions) # output layer w\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
    "        x = self.out(x)         # Calculate output\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Class That Implements Our Deep Q-Network\n",
    "class stock_market_trading_DQN():    \n",
    "    # HyperParameters\n",
    "    alpha = 0.001              # Learning rate\n",
    "    gamma = 0              # Discount Factor\n",
    "    synching_period = 100    # After this many batches we synch the target nn with the policy nn\n",
    "    replay_buffer_size = 10000 # Size of replay buffer\n",
    "    min_batch_size = 64      # Size of each batch\n",
    "    #optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "\n",
    "    # Define Huber as our loss function\n",
    "    # loss_func = nn.SmoothL1Loss()\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    ACTIONS = [0,1]\n",
    "    num_actions = 2\n",
    "    \n",
    "    # Encode the input state \n",
    "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
    "        input_tensor = torch.zeros(num_states)\n",
    "        input_tensor[state] = 1\n",
    "        return input_tensor\n",
    "            \n",
    "    # This method is responsible to train our network based on a number of 'episodes'\n",
    "    def train_DQN(self, episodes,environment,gamma,lr):\n",
    "        P = environment\n",
    "        num_of_states = len(P)\n",
    "        num_of_actions = len(P[0])\n",
    "        \n",
    "        epsilon = 1 # Exploration rate\n",
    "        self.gamma = gamma\n",
    "        self.alpha = lr\n",
    "        memory_buffer = ReplayMemory(self.replay_buffer_size)\n",
    "        #memory_buffer = [[] for _ in range(self.replay_buffer_size)] \n",
    "        \n",
    "        #memory_buffer[i % 1000] = [0,1,2,3]\n",
    "        \n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        # We create a NN with num of input nodes equal to the num of the total states \n",
    "        # The num of output layer nodes is equal to the num of the total actions\n",
    "        # The hidden layer's num of nodes is equal to the num of states -> this is adjustable\n",
    "        policy_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions)\n",
    "        target_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions)\n",
    "\n",
    "        # initialize the 2 networks to be the same \n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        # print('Policy (random, before training):')\n",
    "        # self.print_dqn(policy_dqn)\n",
    "        # print('===============================================================')\n",
    "        # print('===============================================================')\n",
    "\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        \n",
    "        # self.optimizer = torch.optim.RMSprop(policy_dqn.parameters(), lr=self.alpha, alpha=0.99, \n",
    "        #                                      eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.alpha)\n",
    "        # optimizer = SGD([parameter], lr=0.1)\n",
    "        \n",
    "        # keep track of the reward at each round \n",
    "        reward_tracking = np.zeros(episodes)\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_tracking = []\n",
    "        synch_counter = 0 # which step we are on \n",
    "        \n",
    "        progress_bar = tqdm(range(episodes))\n",
    "        for i in progress_bar:\n",
    "            current_state = random.randint(0, len(P)-1) # select a random starting state\n",
    "        \n",
    "            for _ in range(100):      # do 100 steps do get a feel for what happens in the environment\n",
    "                # decide if we are going to explore or to exploit based on the epsilon value\n",
    "                # if random.uniform(0,1) < epsilon:\n",
    "                if random.random() < epsilon:\n",
    "                    #action = np.random.binomial(1,0.5)     # Explore by picking a random action\n",
    "                    action = random.choice([0,1])\n",
    "                else:\n",
    "                     # From the output layer, choose the node output (action) with the maximum value\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(current_state, num_of_states)).argmax().item()\n",
    "                    \n",
    "                # get the response from the environment\n",
    "                next_state,reward = get_response(P, current_state, action)\n",
    "                # reward_tracking[i] = reward\n",
    "                \n",
    "                # Store the environments response into our memory        \n",
    "                # memory_buffer[step % 1000] = [current_state, action, next_state, reward]\n",
    "                memory_buffer.append((current_state, action, next_state, reward)) \n",
    "            \n",
    "                # update the next state\n",
    "                current_state = next_state    \n",
    "            \n",
    "                # Increment step counter\n",
    "                synch_counter += 1\n",
    "            \n",
    "            # Perform the optimization\n",
    "            if(len(memory_buffer) > self.min_batch_size):\n",
    "\n",
    "                #mini_batch = self.sample_mem_buffer(memory_buffer, self.min_batch_size)\n",
    "                mini_batch = memory_buffer.sample(self.min_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                #epsilon = max(epsilon * 0.99, 0.1)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                ### CHECK\n",
    "                # if (step % self.synching_period) == 0:\n",
    "                if synch_counter > self.synching_period :\n",
    "                # if (synch_counter  self.synching_period): \n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    synch_counter = 0\n",
    "\n",
    "        # return the optimal policy\n",
    "        #return policy_dqn.state_dict()\n",
    "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
    "        return policy_dqn\n",
    "                \n",
    "    def optimize(self,mini_batch, policy_dqn, target_dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = policy_dqn.fc1.in_features\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward in mini_batch:\n",
    "            # Calculate target q value \n",
    "            # We disable the gradient tracking for memory optimization\n",
    "            with torch.no_grad():\n",
    "                # Here we get the optimal output we SHOULD have gotten according to the target NN\n",
    "                target = torch.FloatTensor(\n",
    "                    # For DQNs the target NNs parameters are modified according to the equation\n",
    "                    # Q[state,action] = reward + γ *max{Q[next_state]}\n",
    "                    reward + self.gamma * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
    "                )\n",
    "                    \n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state, num_states)) \n",
    "\n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "        # calculate the loss for all the batch  \n",
    "        loss = self.loss_func(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model by running back-propagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Test function\n",
    "    def test_DQN(self, episodes,environment):\n",
    "        # Create FrozenLake instance\n",
    "        P = environment\n",
    "        num_of_states = len(P)\n",
    "        num_of_actions = len(P[0])\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_of_states, h1_nodes=num_of_states, out_actions=num_of_actions) \n",
    " \n",
    "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        # print('Policy (trained):')\n",
    "        # self.print_dqn(policy_dqn)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            current_state = random.randint(0, num_of_states-1)\n",
    "\n",
    "            for _ in range(100):\n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(current_state, num_of_states)).argmax().item()\n",
    "                # Execute action\n",
    "                current_state,reward = get_response(P, current_state, action)\n",
    "\n",
    "        \n",
    "        \n",
    "    def print_dqn(self, dqn):\n",
    "        # Get number of input nodes\n",
    "        num_states = dqn.fc1.in_features\n",
    "        Q_table = np.zeros((num_states, self.num_actions))\n",
    "\n",
    "        # Loop each state and print policy to console\n",
    "        for s in range(num_states):\n",
    "\n",
    "            q_values_element = dqn(self.state_to_dqn_input(s, num_states)).tolist()\n",
    "            Q_table[s] = q_values_element\n",
    "            \n",
    "            #  Format q values for printing\n",
    "            q_values = ''\n",
    "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
    "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
    "            q_values=q_values.rstrip()              # Remove space at the end\n",
    "            #\n",
    "\n",
    "            # Map the best action\n",
    "            best_action = dqn(self.state_to_dqn_input(s, num_states)).argmax()\n",
    "\n",
    "            # Print policy in the format of: state, action, q values\n",
    "            # The printed layout matches the FrozenLake map.\n",
    "            print(f'{s:02},{best_action},[{q_values}]', end='\\n')         \n",
    "            if (s+1)%4==0:\n",
    "                print() # Print a newline every 4 states\n",
    "            \n",
    "        #Q_table_transposed = [list(row) for row in zip(*Q_table)]\n",
    "        return Q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna study for DQN (hyperparameter auto-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-18 18:59:56,323] A new study created in memory with name: no-name-3c8a457c-3128-487a-bb4b-1718f3f0f25c\n",
      "C:\\Users\\vagga\\AppData\\Local\\Temp\\ipykernel_20852\\994877475.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "[W 2024-07-18 18:59:56,330] Trial 0 failed with parameters: {'alpha': 0.0024531534129901156, 'synching_period': 132, 'replay_buffer_size': 4506, 'min_batch_size': 106} because of the following error: TypeError(\"object of type 'numpy.float64' has no len()\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vagga\\AppData\\Local\\Temp\\ipykernel_20852\\994877475.py\", line 19, in objective\n",
      "    trained_policy = dqn_agent.train_DQN(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vagga\\AppData\\Local\\Temp\\ipykernel_20852\\2534460160.py\", line 115, in train_DQN\n",
      "    next_state,reward = get_response(P, current_state, action)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vagga\\AppData\\Local\\Temp\\ipykernel_20852\\292092992.py\", line 59, in get_response\n",
      "    choices = range(len(P[state][action]))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: object of type 'numpy.float64' has no len()\n",
      "[W 2024-07-18 18:59:56,331] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Create a study and optimize the objective function\u001b[39;00m\n\u001b[0;32m     40\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\vagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\vagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\vagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     16\u001b[0m P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(num_states, num_actions)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the DQN with the given hyperparameters\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m trained_policy \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_DQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Test the trained DQN\u001b[39;00m\n\u001b[0;32m     27\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[17], line 115\u001b[0m, in \u001b[0;36mstock_market_trading_DQN.train_DQN\u001b[1;34m(self, episodes, environment, gamma, lr)\u001b[0m\n\u001b[0;32m    112\u001b[0m         action \u001b[38;5;241m=\u001b[39m policy_dqn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_dqn_input(current_state, num_of_states))\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# get the response from the environment\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m next_state,reward \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# reward_tracking[i] = reward\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Store the environments response into our memory        \u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# memory_buffer[step % 1000] = [current_state, action, next_state, reward]\u001b[39;00m\n\u001b[0;32m    120\u001b[0m memory_buffer\u001b[38;5;241m.\u001b[39mappend((current_state, action, next_state, reward)) \n",
      "Cell \u001b[1;32mIn[15], line 59\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(environment, state, action)\u001b[0m\n\u001b[0;32m     55\u001b[0m                             \u001b[38;5;66;03m# based on the current state and the action we want to make   \u001b[39;00m\n\u001b[0;32m     56\u001b[0m \n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# we use random.choices to get a random next state based on the weighted probabilities of the next states\u001b[39;00m\n\u001b[0;32m     58\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 59\u001b[0m choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(P[state][action])): \n\u001b[0;32m     61\u001b[0m     probabilities\u001b[38;5;241m.\u001b[39mappend(response[i][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
    "    # gamma = trial.suggest_uniform('gamma', 0.8, 1.0)\n",
    "    gamma = 0\n",
    "    synching_period = trial.suggest_int('synching_period', 50, 500)\n",
    "    replay_buffer_size = trial.suggest_int('replay_buffer_size', 1000, 20000)\n",
    "    min_batch_size = trial.suggest_int('min_batch_size', 32, 256)\n",
    "    \n",
    "    # Create an instance of the stock_market_trading_DQN class\n",
    "    dqn_agent = stock_market_trading_DQN()\n",
    "    \n",
    "    # Dummy environment\n",
    "    num_states = 16\n",
    "    num_actions = 2\n",
    "    P = np.random.rand(num_states, num_actions)\n",
    "    \n",
    "    # Train the DQN with the given hyperparameters\n",
    "    trained_policy = dqn_agent.train_DQN(\n",
    "        episodes=500,\n",
    "        environment=P,\n",
    "        gamma=gamma,\n",
    "        lr=alpha\n",
    "    )\n",
    "    \n",
    "    # Test the trained DQN\n",
    "    total_reward = 0\n",
    "    for _ in range(10):  # Run multiple episodes for evaluation\n",
    "        current_state = random.randint(0, len(P) - 1)\n",
    "        for _ in range(100):  # Steps in an episode\n",
    "            with torch.no_grad():\n",
    "                action = trained_policy(dqn_agent.state_to_dqn_input(current_state, num_states)).argmax().item()\n",
    "            current_state, reward = get_response(P, current_state, action)\n",
    "            total_reward += reward\n",
    "    \n",
    "    # Return the total reward as the objective to maximize\n",
    "    return total_reward\n",
    "\n",
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 11792.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#environment = P2\n",
    "environment = generate_environment(3,0.001)\n",
    "gamma = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Optimal Policy (policy Iteration -> Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 Policy Iterations\n",
      "[1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "V_opt1,P_opt1,Q_opt = policy_iteration(environment,gamma)\n",
    "print(np.argmax(Q_opt,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Tubular Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create Optuna study\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# study = optuna.create_study(direction='minimize')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#     Q_tubular,_ = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     print(f\"\\n {i} FINAL OPTIMAL POLICY {np.argmax(Q_tubular,axis=1)}\")\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m Q_tubular,_ \u001b[38;5;241m=\u001b[39m \u001b[43mimplement_Q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(Q_opt,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFINAL OPTIMAL POLICY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39margmax(Q_tubular,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m, in \u001b[0;36mimplement_Q_learning\u001b[1;34m(env, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\u001b[0m\n\u001b[0;32m     38\u001b[0m actions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     39\u001b[0m action \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices([random\u001b[38;5;241m.\u001b[39mchoices(actions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39margmax(Q[current_state])], weights\u001b[38;5;241m=\u001b[39m[epsilon, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon], k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 41\u001b[0m next_state,reward \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#print(f\"Next State {next_state}\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m Q[current_state,action] \u001b[38;5;241m=\u001b[39m Q[current_state,action] \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m     45\u001b[0m     reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q[next_state]) \u001b[38;5;241m-\u001b[39m Q[current_state,action]\n\u001b[0;32m     46\u001b[0m )\n",
      "Cell \u001b[1;32mIn[23], line 63\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(environment, state, action)\u001b[0m\n\u001b[0;32m     58\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# choices = range(len(P[state][action]))\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# for i in range(len(P[state][action])): \u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m#     probabilities.append(response[i][0])\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m choices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(P[state][\u001b[38;5;241m2\u001b[39m])): \n\u001b[0;32m     65\u001b[0m     probabilities\u001b[38;5;241m.\u001b[39mappend(response[i][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create Optuna study\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=40)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print('Best hyperparameters: ', study.best_params)\n",
    "\n",
    "# optimal_alpha = study.best_params['alpha']\n",
    "# optimal_epsilon_decay = study.best_params['epsilon_decay']\n",
    "# optimal_alpha_decay = study.best_params['alpha_decay']\n",
    "\n",
    "\n",
    "# for i in range(10):\n",
    "#     environment = environment\n",
    "#     num_of_episodes = 50000\n",
    "#     alpha = optimal_alpha\n",
    "#     #gamma = 0\n",
    "#     epsilon_decay = optimal_epsilon_decay\n",
    "#     alpha_decay = optimal_alpha_decay\n",
    "#     finding_parameters =  False\n",
    "#     Q_tubular,_ = implement_Q_learning(environment, num_of_episodes, alpha, gamma, epsilon_decay, alpha_decay, finding_parameters)\n",
    "#     print(f\"\\n {i} FINAL OPTIMAL POLICY {np.argmax(Q_tubular,axis=1)}\")\n",
    "\n",
    "\n",
    "Q_tubular,_ = implement_Q_learning(environment, num_of_episodes=20000, alpha=0.5, gamma=gamma)\n",
    "print(np.argmax(Q_opt,axis=1))\n",
    "print(f\"\\nFINAL OPTIMAL POLICY {np.argmax(Q_tubular,axis=1)}\")\n",
    "print(\"Qopt = \",Q_opt)\n",
    "print(\"\\nQtabular = \",Q_tubular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Run the DQN for the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:01<00:00, 55.20it/s]\n"
     ]
    }
   ],
   "source": [
    "num_of_episodes = 10000\n",
    "NN_learning_rate = 0.01\n",
    "dql = stock_market_trading_DQN()\n",
    "optimal_network = dql.train_DQN(num_of_episodes,environment,gamma,NN_learning_rate)\n",
    "dql.test_DQN(10,environment)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Optimal Policies Generated from diffrent algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 Optimal Policy\n",
      "State 0: Action 0\n",
      "State 1: Action 1\n",
      "State 2: Action 1\n",
      "State 3: Action 1\n",
      "State 4: Action 1\n",
      "State 5: Action 0\n",
      "State 6: Action 1\n",
      "State 7: Action 1\n",
      "State 8: Action 0\n",
      "State 9: Action 1\n",
      "State 10: Action 0\n",
      "State 11: Action 1\n",
      "State 12: Action 1\n",
      "State 13: Action 1\n",
      "State 14: Action 1\n",
      "State 15: Action 0\n",
      "State 16: Action 1\n",
      "State 17: Action 1\n",
      "State 18: Action 1\n",
      "State 19: Action 1\n",
      "State 20: Action 1\n",
      "State 21: Action 1\n",
      "State 22: Action 1\n",
      "State 23: Action 1\n",
      "\n",
      "Optimal Q =  [[0.03031047 0.02815876]\n",
      " [0.02638227 0.13133163]\n",
      " [0.04941365 0.05844365]\n",
      " [0.01009096 0.02041519]\n",
      " [0.01933133 0.10841806]\n",
      " [0.08369228 0.0101682 ]\n",
      " [0.02019804 0.0914925 ]\n",
      " [0.06360484 0.13300494]\n",
      " [0.06827976 0.03619543]\n",
      " [0.04675142 0.10075909]\n",
      " [0.01271144 0.01238558]\n",
      " [0.05064312 0.10624941]\n",
      " [0.00558377 0.09212666]\n",
      " [0.07747172 0.12413358]\n",
      " [0.00161899 0.12226172]\n",
      " [0.07114383 0.03561536]\n",
      " [0.05897908 0.07234668]\n",
      " [0.04094774 0.10078331]\n",
      " [0.02838915 0.076867  ]\n",
      " [0.00857494 0.04552349]\n",
      " [0.03768553 0.06137097]\n",
      " [0.04900211 0.07023331]\n",
      " [0.04102116 0.07171289]\n",
      " [0.03395023 0.06649623]]\n",
      "================================================================\n",
      "Phase 2 - DQN Optimal Policy\n",
      "00,1,[+0.03 +0.03]\n",
      "01,1,[+0.05 +0.07]\n",
      "02,0,[+0.06 +0.03]\n",
      "03,1,[-0.01 +0.01]\n",
      "\n",
      "04,1,[+0.05 +0.06]\n",
      "05,0,[+0.09 +0.02]\n",
      "06,1,[+0.03 +0.05]\n",
      "07,1,[+0.07 +0.07]\n",
      "\n",
      "08,0,[+0.08 +0.02]\n",
      "09,1,[+0.05 +0.05]\n",
      "10,0,[+0.02 +0.00]\n",
      "11,1,[+0.05 +0.06]\n",
      "\n",
      "12,1,[+0.02 +0.05]\n",
      "13,0,[+0.08 +0.05]\n",
      "14,1,[+0.05 +0.06]\n",
      "15,0,[+0.08 +0.02]\n",
      "\n",
      "16,0,[+0.06 +0.03]\n",
      "17,1,[+0.05 +0.06]\n",
      "18,0,[+0.04 +0.04]\n",
      "19,1,[+0.01 +0.02]\n",
      "\n",
      "20,0,[+0.04 +0.04]\n",
      "21,0,[+0.04 +0.04]\n",
      "22,0,[+0.04 +0.04]\n",
      "23,0,[+0.04 +0.04]\n",
      "\n",
      "================================================================\n",
      "\n",
      "Difference With NN\n",
      "[[ 0.02589574  0.02893985]\n",
      " [ 0.05127185  0.07035669]\n",
      " [ 0.05900951  0.0304405 ]\n",
      " [-0.00709187  0.01241054]\n",
      " [ 0.04813159  0.06101475]\n",
      " [ 0.09477926  0.02045076]\n",
      " [ 0.03173865  0.0464237 ]\n",
      " [ 0.06785353  0.07292898]\n",
      " [ 0.07958704  0.02469363]\n",
      " [ 0.04605915  0.05484944]\n",
      " [ 0.02245287  0.00468479]\n",
      " [ 0.04693208  0.0574463 ]\n",
      " [ 0.02254101  0.04918419]\n",
      " [ 0.08455123  0.05466224]\n",
      " [ 0.04840245  0.06182052]\n",
      " [ 0.08302356  0.02373388]\n",
      " [ 0.06308224  0.02930307]\n",
      " [ 0.04807255  0.06083909]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.01030549  0.02112795]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.03966977  0.03584168]\n",
      " [ 0.03966977  0.03584168]]\n",
      "difference [[0.004414725793680158, -0.0007810905999239906], [-0.02488958658016814, 0.060974937856247], [-0.00959585312111206, 0.028003146499589057], [0.01718283170164258, 0.00800464607722818], [-0.02880025840334086, 0.04740330672671676], [-0.011086982461637501, -0.010282554160741676], [-0.011540608280792505, 0.04506880033315021], [-0.0042486864602547525, 0.06007596399039533], [-0.01130727949207691, 0.01150179965395804], [0.000692266226961967, 0.04590965208282173], [-0.009741431829398992, 0.007700791122763636], [0.003711045116518291, 0.048803112730448156], [-0.01695723605564086, 0.04294247585317268], [-0.007079505177956524, 0.06947134331745919], [-0.046783465581209244, 0.060441201296219924], [-0.011879722529648293, 0.0118814796998171], [-0.004103159745982118, 0.043043610352951595], [-0.007124805208642962, 0.039944226142635805], [-0.011280613047378096, 0.041025315913450905], [-0.0017305584422063931, 0.02439554568868116], [-0.001984240203642025, 0.025529284980825774], [0.009332344650486858, 0.03439162789604554], [0.0013513927024115385, 0.035871205827361446], [-0.005719539883460162, 0.030654540287958082]]\n",
      "Total Error: 0.0008927272074833766\n"
     ]
    }
   ],
   "source": [
    "# Phase 1 Optimal Policy\n",
    "print(\"Phase 1 Optimal Policy\")\n",
    "print_policy(P_opt1,len(environment))\n",
    "print(\"\\nOptimal Q = \",Q_opt)\n",
    "print(\"================================================================\")\n",
    "# Phase 2 - Tabular Q-Learning Optimal Policy\n",
    "# print(\"Phase 2 - Tubular Q-Learning Optimal Policy\")\n",
    "# print(np.argmax(Q_tubular,axis=1))\n",
    "# print(\"================================================================\")\n",
    "\n",
    "# Phase 2 - DQN Optimal Policy\n",
    "print(\"Phase 2 - DQN Optimal Policy\")\n",
    "Q_NN = dql.print_dqn(optimal_network)\n",
    "print(\"================================================================\")\n",
    "\n",
    "# Output difference\n",
    "# print(\"\\nDifference With Tabular\")\n",
    "# difference,total_error = calculate_difference_and_mse(Q_opt,Q_tubular)\n",
    "# print(f\"difference {difference}\\nTotal Error: {total_error}\")\n",
    "\n",
    "# Output difference\n",
    "print(\"\\nDifference With NN\")\n",
    "print(Q_NN)\n",
    "difference,total_error = calculate_difference_and_mse(Q_opt,Q_NN)\n",
    "print(f\"difference {difference}\\nTotal Error: {total_error}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
